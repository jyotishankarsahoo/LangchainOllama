{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2034c969",
   "metadata": {},
   "source": [
    "#### Initial Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af3c9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b268693",
   "metadata": {},
   "source": [
    "#### Define 2 LLms to run in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f89aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "qwen_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                       model=\"qwen2.5:latest\",\n",
    "                       temperature=0.5,\n",
    "                       max_tokens=250)\n",
    "llama_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                       model=\"Llama3.2:latest\",\n",
    "                       temperature=0.5,\n",
    "                       max_tokens=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd54d5",
   "metadata": {},
   "source": [
    "#### Create Prompt Template for Both the Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7b8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "local_machine_prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of using LLM in {env}\")\n",
    "])\n",
    "cloud_machine_prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                                                 What is the advantage of using LLM in {machine}\n",
    "                                                                 \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f081bd",
   "metadata": {},
   "source": [
    "#### Create 2 Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9031aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "local_machine_chain = local_machine_prompt_template | qwen_llm | StrOutputParser()\n",
    "cloud_machine_chain = cloud_machine_prompt_template | llama_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545827",
   "metadata": {},
   "source": [
    "#### Create Runnable Parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bff0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "runnable_chain = RunnableParallel(chain1=local_machine_chain, chain2=cloud_machine_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249936b",
   "metadata": {},
   "source": [
    "#### Invocation of Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7c134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a Large Language Model (LLM) on a local machine offers several advantages, including:\n",
      "\n",
      "1. **Privacy and Security**: By processing data locally, you avoid sending sensitive information to remote servers, which can enhance privacy and security.\n",
      "\n",
      "2. **Latency Reduction**: Local processing reduces the latency associated with network communication, leading to faster response times for applications that rely on LLMs.\n",
      "\n",
      "3. **Offline Use**: You can use the model even when there is no internet connection, making it more versatile in various usage scenarios.\n",
      "\n",
      "4. **Customization and Control**: Having the LLM locally allows you to customize and control its behavior without relying on external services or APIs, which might have their own terms of service or limitations.\n",
      "\n",
      "5. **Resource Management**: You can optimize resource allocation (CPU, memory) for the model more efficiently when it is running locally, as you have full control over the hardware resources.\n",
      "\n",
      "6. **Scalability and Performance**: For certain tasks, local execution can be optimized better than cloud-based services, especially if you need to process large amounts of data in real-time.\n",
      "\n",
      "7. **Cost Efficiency**: Depending on your use case, using a local LLM might be more cost-effective compared to paying for cloud services or API calls.\n",
      "\n",
      "However, it's important to note that running an LLM locally also comes with challenges such as the need for significant computational resources and storage space, as well as the complexity of maintaining and updating the model.\n",
      "\n",
      "\n",
      "\n",
      "The advantages of using Large Language Models (LLMs) in the cloud are numerous:\n",
      "\n",
      "1. **Scalability**: Cloud-based LLMs can scale horizontally, allowing for seamless deployment and scaling to meet increasing demand.\n",
      "2. **Cost-effectiveness**: Pay-as-you-go pricing models enable organizations to only pay for the compute resources they use, reducing costs associated with maintaining on-premises infrastructure.\n",
      "3. **High-performance computing**: Cloud providers offer high-performance computing (HPC) capabilities, enabling LLMs to process vast amounts of data quickly and efficiently.\n",
      "4. **Access to advanced hardware**: Cloud-based LLMs can utilize advanced hardware such as GPUs, TPUs, and FPGAs, which accelerate training and inference processes.\n",
      "5. **Distributed training**: Cloud environments allow for distributed training, enabling simultaneous training on multiple machines, reducing training time, and improving model performance.\n",
      "6. **Collaboration and sharing**: Cloud-based LLMs facilitate collaboration among researchers, developers, and organizations, promoting knowledge sharing and accelerating progress in the field.\n",
      "7. **Automatic updates and maintenance**: Cloud providers handle software updates, maintenance, and security patches, ensuring that LLMs stay up-to-date and secure.\n",
      "8. **Global access to data**: Cloud-based LLMs can tap into vast amounts of global data, enabling organizations to leverage diverse datasets and improve model performance.\n",
      "9. **Flexibility and portability**: Cloud-based LLMs can be easily deployed on various platforms, including containers, virtual machines, or bare metal instances, making them highly portable.\n",
      "10. **Reduced infrastructure requirements**: Cloud-based LLMs eliminate the need for organizations to manage their own infrastructure, reducing the administrative burden and costs associated with maintaining hardware and software.\n",
      "\n",
      "Some popular cloud providers for LLMs include:\n",
      "\n",
      "* Amazon Web Services (AWS)\n",
      "* Microsoft Azure\n",
      "* Google Cloud Platform (GCP)\n",
      "* IBM Cloud\n",
      "\n",
      "These cloud providers offer a range of services, including:\n",
      "\n",
      "* Compute instances (e.g., EC2, VMs)\n",
      "* Storage solutions (e.g., S3, Blob Storage)\n",
      "* Networking and security services (e.g., VPC, IAM)\n",
      "* AI and machine learning services (e.g., SageMaker, AutoML)\n",
      "\n",
      "By leveraging the advantages of cloud-based LLMs, organizations can accelerate their development, deployment, and maintenance processes, ultimately driving innovation and progress in the field.\n"
     ]
    }
   ],
   "source": [
    "response = runnable_chain.invoke({\"env\": \"Local Machine\", \"machine\": \"Cloud\"})\n",
    "print(response[\"chain1\"])\n",
    "print(\"\\n\\n\")\n",
    "print(response[\"chain2\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
