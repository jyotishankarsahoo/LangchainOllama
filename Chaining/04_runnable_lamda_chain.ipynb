{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3888933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be1511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "qwen_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                       model=\"qwen2.5:latest\",\n",
    "                       temperature=0.5,\n",
    "                       max_tokens=250)\n",
    "llama_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                       model=\"Llama3.2:latest\",\n",
    "                       temperature=0.5,\n",
    "                       max_tokens=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b004cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt1 = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an expert on LLMs.\"),\n",
    "    (\"user\", \"What are the advantages of using LLMs in {env}?\"),\n",
    "])\n",
    "prompt2 = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                           Analyse the response and get me just headings from {response}?\n",
    "                                           \n",
    "                                           Response should be in bullet points.\n",
    "                                           \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2809f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using RunnableLambda to choose LLM based on response length\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "def choose_llm(response_str) -> ChatOllama:\n",
    "    if len(str(response_str)) > 300:\n",
    "        return llama_llm\n",
    "    return qwen_llm\n",
    "\n",
    "runnable_llm = RunnableLambda(choose_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9317f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Here are the headings from the response:\n",
      "\n",
      "* Advantages of Using Large Language Models (LLMs) Locally\n",
      "  • Enhanced Privacy\n",
      "  • Reduced Latency\n",
      "  • Offline Capabilities\n",
      "  • Cost Efficiency\n",
      "  • Customization and Control\n",
      "  • Scalability\n",
      "  • Compliance and Regulations\n",
      "  • Performance Optimization\n",
      "  • Security\n",
      "  • Real-Time Analysis\n",
      "\n",
      "* Considerations for Effective Deployment\n",
      "  • Computational Requirements\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain1 = prompt1 | runnable_llm | StrOutputParser()\n",
    "chain2 = {\"response\": chain1} | prompt2 | runnable_llm | StrOutputParser()\n",
    "response = chain2.invoke({\"env\": \"Local System\"})\n",
    "print(\"Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e49579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using @Chain decorator to define the chain\n",
    "from langchain_core.runnables import chain\n",
    "@chain\n",
    "def choose_llm(response_str) -> ChatOllama:\n",
    "    if len(str(response_str)) > 300:\n",
    "        return llama_llm\n",
    "    return qwen_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b949ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Here are the headings from the response:\n",
      "\n",
      "• **Privacy and Security**\n",
      "• **Latency Reduction**\n",
      "• **Offline Capabilities**\n",
      "• **Customization and Control**\n",
      "• **Cost Efficiency**\n",
      "• **Scalability**\n",
      "• **Compliance and Regulations**\n",
      "• **Performance Optimization**\n",
      "• **Resource Management**\n",
      "• **Custom Training and Fine-Tuning**\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain1 = prompt1 | qwen_llm | StrOutputParser()\n",
    "chain2 = {\"response\": chain1} | prompt2 | choose_llm | StrOutputParser()\n",
    "response = chain2.invoke({\"env\": \"Local System\"})\n",
    "print(\"Response:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
