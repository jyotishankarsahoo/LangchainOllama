{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a23130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f65971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM Object\n",
    "from langchain_ollama import ChatOllama\n",
    "local_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                       model=\"qwen2.5:latest\",\n",
    "                       temperature=0.5,\n",
    "                       max_token=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fc974d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Using Large Language Models (LLMs) locally on your own machine offers several advantages, including:\\n\\n1. **Privacy and Security**: By running the model locally, you avoid sending sensitive data to external servers, which can be particularly important for handling personal or confidential information.\\n\\n2. **Latency Reduction**: Local execution reduces latency compared to cloud-based services because there's no need to wait for network requests to process your input. This is especially beneficial for real-time applications where quick responses are crucial.\\n\\n3. **Cost Efficiency**: Running models locally can be more cost-effective, as you don't have to pay for cloud services or API calls. However, this depends on the hardware and software resources available on your machine.\\n\\n4. **Customization and Control**: With local deployment, you have full control over how the model is used and configured. You can tailor its behavior to specific needs without relying on third-party constraints.\\n\\n5. **Offline Use**: Local models can be used even when there's no internet connection, making them suitable for scenarios where network availability is unreliable or restricted.\\n\\n6. **Scalability**: If you have powerful hardware, a single local machine can handle multiple requests simultaneously, potentially offering better scalability compared to cloud-based solutions that might introduce bottlenecks with high concurrency.\\n\\n7. **Performance Optimization**: You can optimize the model's performance by fine-tuning it on specific datasets or by using specialized hardware like GPUs or TPUs if they are available locally.\\n\\n8. **Regulatory Compliance**: In certain industries, such as healthcare and finance, there may be regulatory requirements to process data locally to ensure compliance with data protection laws.\\n\\n9. **Resource Management**: You have more control over the resources allocated to the model, allowing for better management of computational power, memory, and other system resources.\\n\\nHowever, it's important to note that local deployment also comes with challenges such as higher initial setup costs (for powerful hardware), potential complexity in deployment and maintenance, and the need to manage updates and security patches yourself.\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-12-09T15:26:00.212348Z', 'done': True, 'done_reason': 'stop', 'total_duration': 24453918125, 'load_duration': 106312000, 'prompt_eval_count': 31, 'prompt_eval_duration': 157682209, 'eval_count': 404, 'eval_duration': 21428412076, 'logprobs': None, 'model_name': 'qwen2.5:latest', 'model_provider': 'ollama'}, id='lc_run--019b03b7-cc8a-70d2-8116-e0aa021dd1cb-0', usage_metadata={'input_tokens': 31, 'output_tokens': 404, 'total_tokens': 435})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Chat Prompt\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    "    )\n",
    "system_message = SystemMessagePromptTemplate.from_template(\"You are a expert on LLM\")\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"What is the advantage of using LLM on {env}\")\n",
    "chat_prompt_template = ChatPromptTemplate([system_message, human_message])\n",
    "chat_prompt = chat_prompt_template.invoke({\"env\": \"Local Machine\"})\n",
    "local_llm.invoke(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff32f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Using Large Language Models (LLMs) on a local machine has several advantages, particularly in terms of privacy, control, and performance. Here are some key benefits:\\n\\n1. **Privacy**:\\n   - **Data Security**: By running an LLM locally, you don't need to send sensitive data over the internet, which can reduce the risk of data breaches.\\n   - **Compliance**: Local models can be used in environments where strict compliance with data privacy regulations is required.\\n\\n2. **Control and Customization**:\\n   - **Custom Training**: You can fine-tune the model on your own dataset to better suit specific needs or industries, potentially improving performance.\\n   - **Model Updates**: You have full control over when and how updates are applied, which can be crucial in regulated environments where strict versioning is necessary.\\n\\n3. **Performance**:\\n   - **Latency Reduction**: Local processing typically reduces latency compared to cloud-based services, as there's no need for network requests.\\n   - **Resource Utilization**: You can optimize the use of local resources (CPU, GPU) more efficiently since you have full control over resource allocation.\\n\\n4. **Offline Capabilities**:\\n   - **No Internet Dependency**: The model works even when an internet connection is unavailable, making it suitable for scenarios where connectivity is unreliable or limited.\\n   - **Batch Processing**: You can process large datasets without the need for real-time network communication.\\n\\n5. **Cost Efficiency**:\\n   - **Reduced Cloud Costs**: If you only use local models during certain times (e.g., when connected to a high-speed internet), it can reduce cloud usage and associated costs.\\n   - **No Subscription Fees**: Some LLMs might require subscription fees for cloud services, whereas using them locally could be more cost-effective.\\n\\n6. **Security**:\\n   - **Data Encryption**: You can implement robust data encryption measures since the data never leaves your local environment.\\n   - **Secure APIs**: If you need to interact with other systems, you can create secure, local APIs that handle data more safely than cloud-based services.\\n\\n7. **Testing and Development**:\\n   - **Iterative Testing**: Easier to test and iterate on models without the overhead of cloud infrastructure.\\n   - **Development Speed**: Faster development cycles as there's no need to wait for cloud service availability or performance issues.\\n\\n8. **Scalability**:\\n   - **Resource Scaling**: You can scale resources up or down based on local needs, which might be more efficient than scaling a cloud-based model.\\n\\nWhile these advantages are significant, it's important to consider the potential downsides such as increased hardware requirements and the complexity of managing local infrastructure. However, for many use cases, especially those involving sensitive data or high performance requirements, running LLMs locally can provide substantial benefits.\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-12-09T15:26:42.013264Z', 'done': True, 'done_reason': 'stop', 'total_duration': 35146234375, 'load_duration': 107807125, 'prompt_eval_count': 31, 'prompt_eval_duration': 136773417, 'eval_count': 570, 'eval_duration': 30932610462, 'logprobs': None, 'model_name': 'qwen2.5:latest', 'model_provider': 'ollama'}, id='lc_run--019b03b8-460f-7d13-b798-9fd7a198d78a-0', usage_metadata={'input_tokens': 31, 'output_tokens': 570, 'total_tokens': 601})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Chaining \n",
    "chain = chat_prompt_template | local_llm\n",
    "chain.invoke({\"env\": \"Local Machine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cdc6f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Using Large Language Models (LLMs) on a local machine can offer several advantages, depending on your specific use case. Here are some key benefits:\\n\\n1. **Control and Privacy**: Running an LLM locally means you have full control over the data it processes, which is particularly important if you're dealing with sensitive or personal information.\\n\\n2. **Latency Reduction**: Local execution reduces latency by eliminating network delays associated with cloud-based services. This can be crucial for real-time applications where response times are critical.\\n\\n3. **Cost Efficiency**: For small to medium-sized tasks, running an LLM locally might be more cost-effective than using cloud services, which often charge based on usage.\\n\\n4. **Offline Access**: You can use the model even when there is no internet connection, making it suitable for scenarios where connectivity is unreliable or prohibited.\\n\\n5. **Customization and Integration**: Local deployment allows you to customize the LLM according to your specific needs and integrate it with other local software systems more seamlessly.\\n\\n6. **Security**: By keeping the data and the model on a local machine, you reduce the risk of data breaches and unauthorized access that can occur when using cloud services.\\n\\n7. **Scalability**: For smaller-scale applications or prototyping, local deployment can be easier to scale up or down without the overhead of managing cloud resources.\\n\\n8. **Resource Management**: You have direct control over how system resources (CPU, memory) are allocated and utilized by the LLM, which can help optimize performance.\\n\\nHowever, it's important to note that running a large language model locally requires significant computational resources. Models like those from providers such as Anthropic, Stability AI, or those based on frameworks like Hugging Face's transformers need substantial GPU power and memory to operate efficiently.\\n\\nIn summary, while local deployment of LLMs offers several benefits in terms of control, privacy, and performance, it also comes with the challenge of resource management.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a parser to the Chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = chat_prompt_template | local_llm | StrOutputParser()\n",
    "chain.invoke({\"env\": \"Local Machine\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
