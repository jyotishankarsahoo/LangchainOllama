{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0aeeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc25d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llama_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                       model=\"llama3.2:latest\", \n",
    "                       temperature=0.7,\n",
    "                       max_tokens=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313de8e",
   "metadata": {},
   "source": [
    "#### Load PDF into Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "509d2fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "pdf_1 = PyPDFLoader(\"./SampleData/attention.pdf\")\n",
    "pdf_2 = PyPDFLoader(\"./SampleData/LLMForgetting.pdf\")\n",
    "pdf_3 = PyPDFLoader(\"./SampleData/TestingAndEvaluatingLLM.pdf\")\n",
    "pdf_files = [pdf_1, pdf_2, pdf_3]\n",
    "documents = []\n",
    "for loader in pdf_files:\n",
    "    documents.extend(loader.load())\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83284946",
   "metadata": {},
   "source": [
    "#### Divide Large Document in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a06fc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                               chunk_overlap=200,\n",
    "                                               add_start_index=True)\n",
    "all_split = text_splitter.split_documents(documents)\n",
    "len(all_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73b78f",
   "metadata": {},
   "source": [
    "#### Embedding \n",
    "- Convert Chunks of Text in to Vector using Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d76adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "ollama_embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vector_1 = ollama_embedding.embed_query(all_split[0].page_content)\n",
    "vector_2 = ollama_embedding.embed_query(all_split[1].page_content)\n",
    "assert len(vector_1) == len(vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764487f",
   "metadata": {},
   "source": [
    "#### Storing Embedded Vectors in the Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3fbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "chroma_db = Chroma.from_documents(documents=all_split,\n",
    "                                  embedding=ollama_embedding,\n",
    "                                  persist_directory=\"./chroma_langchain_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c102c3",
   "metadata": {},
   "source": [
    "#### Retrieval From Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(persist_directory=\"./chroma_langchain_db\", \n",
    "                      embedding_function=ollama_embedding)\n",
    "# Test Document Retrieval\n",
    "vector_store.similarity_search(\"What is bias Testing\", k=3)\n",
    "vector_store.similarity_search_with_score(\"What is bias Testing\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc4439",
   "metadata": {},
   "source": [
    "#### Using Retriever From Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c259c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Retriever for data retrieval\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1}\n",
    ")\n",
    "retriever.batch(\n",
    "    [\n",
    "        \"What is Bias Measurement\",\n",
    "        \"How to test human safety against LLM\",\n",
    "        \"How LLM forgets the context\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd96150",
   "metadata": {},
   "source": [
    "#### Document Retrieval Manual Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aabc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To test translation in Large Language Models (LLMs), you can follow these steps:\\n\\n1. **Prepare a dataset**: Collect a list of sentences or phrases in multiple languages that need to be translated.\\n2. **Use Google Translate**: Translate the original text into English using Google Translate.\\n3. **Modify the prompt**: Add a new sentence to the query prompt, \"Answering starts with \\'Yes\\' or \\'No\\'\", to provide more specific guidance to the LLM.\\n4. **Evaluate the model**: Feed the query prompt and the translated response to the LLM (in this case, ChatGPT) for evaluation.\\n\\nExample:\\n\\nPerson 1: {\"Translate this sentence from French to English.\"}\\nPerson 2: {\"Le chat est noir.\"} (translated to \"The cat is black.\")\\n\\nNote that this approach assumes the LLM can accurately translate and understand the original text. However, I don\\'t know if there are specific testing protocols or methodologies for evaluating translation capabilities in LLMs.\\n\\n**MD Summary**\\n# Testing Translation in LLM\\n## Steps\\n1. Prepare a dataset of sentences or phrases to be translated.\\n2. Use Google Translate to translate the original text into English.\\n3. Modify the query prompt with \"Answering starts with \\'Yes\\' or \\'No\\'\".\\n4. Evaluate the model by feeding the query prompt and translated response.\\n\\n# Limitation\\nI don\\'t know if there are specific testing protocols or methodologies for evaluating translation capabilities in LLMs.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "query = \"How to test Translation in LLM?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an AI Assistant, use the following Context to answer the question correctly.\n",
    "    If you don't know answer, Just tell I don't know\n",
    "    \n",
    "    Also Summarize the response in Markdown Format\n",
    "    \n",
    "    \"context: {context} \\n\\n\"\n",
    "    \"question: {question} \\n\\n\"\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llama_llm | StrOutputParser()\n",
    "response = chain.invoke({\"context\": context_text, \"question\": query})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea0a83",
   "metadata": {},
   "source": [
    "#### Using Langchain Client for Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817db035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To test translation in a Large Language Model (LLM), you can use the following method: \\n\\n1. Provide a prompt sentence to the LLM.\\n2. Translate the LLM's response into English using Google Translate.\\n3. Evaluate the translated response for safety and accuracy.\\n\\nThis method allows you to assess how well the LLM performs in different languages by translating its responses back into English.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langsmith import Client\n",
    "\n",
    "query = \"How to test translation in LLM?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "hub = Client()\n",
    "prompt = hub.pull_prompt(\"rlm/rag-prompt\")\n",
    "\n",
    "chain = prompt | llama_llm | StrOutputParser()\n",
    "chain.invoke({\"context\": context_text, \"question\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74f9c4",
   "metadata": {},
   "source": [
    "#### Retrieve Data using Custom Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4851ede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## `Biased` refers to the presence of skewed or imbalanced information in a Language Model's (LLM) knowledge base.\\n### Example: In the context of question-answering tasks, biased knowledge might result in inaccurate or incomplete responses.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "system_prompt = (\n",
    "    \"You are an assistance for question-answering task\"\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "    \"Use three sentences maximum to answer the question and keep the answer concise.\"\n",
    "    \"The answer should be in markdown format.\"\n",
    "    \"\\n\\n{context}\\n\\n\"\n",
    ")\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    } \n",
    "    | prompt_template \n",
    "    | llama_llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"What is Bias in LLM?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
