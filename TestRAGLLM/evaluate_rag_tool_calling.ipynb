{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a77897c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc66ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "qwen_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                      model=\"qwen3:8b\",\n",
    "                      temperature=0,\n",
    "                      max_tokens = 250)\n",
    "llama_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                      model=\"llama3.2:latest\",\n",
    "                      temperature=0,\n",
    "                      max_tokens = 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5d3ad",
   "metadata": {},
   "source": [
    "#### Define Vector Store from persistent Storage and Retriever  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a316aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b208cb73-c426-4bec-a2bd-65a5daea3565', metadata={'creationdate': '2025-01-07T01:36:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': '', 'source': './SampleData/LLMForgetting.pdf', 'creator': 'LaTeX with hyperref', 'page_label': '10', 'page': 9, 'keywords': '', 'subject': '', 'producer': 'pdfTeX-1.40.25', 'moddate': '2025-01-07T01:36:50+00:00', 'total_pages': 15, 'trapped': '/False', 'author': '', 'start_index': 0}, page_content='Under review\\nFigure 6: The performance of general knowledge of the BLOOMZ-7.1b and LLAMA-7b\\nmodel trained on the instruction data and the mixed data. The dashed lines refers to the\\nperformance of BLOOMZ-7.1b and LLAMA-7B and the solid ones refer to those of mixed-\\ninstruction trained models.\\nincreases to 3b, BLOOMZ-3b suffers less forgetting compared to mT0-3.7B. For example, the\\nFG value of BLOOMZ-3b is 11.09 which is 5.64 lower than that of mT0-3.7b. These results\\nsuggest that BLOOMZ, which has a decoder-only model architecture, can maintain more\\nknowledge during continual instruction tuning. This difference may be attributed to the\\nautoregressive nature of the model or the differences in training objectives. Furthermore,\\nthe results imply that as the model scale increases, decoder-only models may suffer from\\nless catastrophic forgetting compared to encoder-decoder models. As we observe, the\\nknowledge degraded more drastically in mT0.\\n5.4 Effect of General Instruction Tuning'),\n",
       " Document(id='26d1a778-f6d2-4438-a671-941f7df0130c', metadata={'start_index': 0, 'creationdate': '2024-09-04T00:37:21+00:00', 'moddate': '2024-09-04T00:37:21+00:00', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'page_label': '144', 'subject': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'keywords': '', 'source': './SampleData/TestingAndEvaluatingLLM.pdf', 'total_pages': 223, 'page': 162, 'author': '', 'trapped': '/False', 'title': ''}, page_content='144 CHAPTER 7. SOCIAL BIAS\\nThe third threat lies in the conversational AI systems used in\\nthe evaluation. We do not evaluate the performance of BiasAsker on\\nother systems. To mitigate this threat, we chose to test commercial\\nconversational systems and SOTA academic models provided by big\\ncompanies. In the future, we could test more commercial software and\\nresearch models to further mitigate this threat.\\n7.4.2 Conclusion\\nIn this chapter, we design and implement BiasAsker, the first au-\\ntomated testing framework for comprehensively measuring the social\\nbiases in conversational AI systems. BiasAsker is able to evaluate 1) to\\nwhat degree is the system biased and 2) how social groups and biased\\nproperties are associated in the system. We conduct experiments\\non eight widely deployed commercial conversational AI systems and\\ntwo famous research models and demonstrate that BiasAsker can\\neffectively trigger a massive amount of biased behavior.\\n7.4.3 Limitations'),\n",
       " Document(id='5209807c-a7cc-4303-8896-f609fea10222', metadata={'source': './SampleData/TestingAndEvaluatingLLM.pdf', 'page_label': '60', 'page': 78, 'start_index': 0, 'trapped': '/False', 'keywords': '', 'title': '', 'creationdate': '2024-09-04T00:37:21+00:00', 'moddate': '2024-09-04T00:37:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with hyperref', 'total_pages': 223, 'producer': 'pdfTeX-1.40.25', 'author': '', 'subject': ''}, page_content='60 CHAPTER 4. LOGICAL REASONING CORRECTNESS\\nthe following challenges: 1) If an LLM concludes correctly, it is unclear\\nwhether the response stems from reasoning or merely relies on simple\\nheuristics such as memorization or word correlations (e.g., “dry floor”\\nis more likely to correlate with “playing football”). 2) If an LLM\\nfails to reason correctly, it is not clear which part of the reasoning\\nprocess it failed (i.e., inferring not raining from floor being dry or\\ninferring playing football from not raining). 3) There is a lack of\\na system that can organize such test cases to cover all other formal\\nreasoning scenarios besides implication, such as logical equivalence\\n(e.g., If A then B, if B then A; therefore, A if and only if B). 4)\\nFurthermore, understanding an LLM’s performance on such test cases\\nprovides little guidance on improving the reasoning ability of the\\nLLM. To better handle these challenges, a well-performing testing')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "ollama_embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vector_store = Chroma(persist_directory=\"./../RagAgent/chroma_langchain_db\",\n",
    "                      embedding_function=ollama_embedding)\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\": 3}\n",
    ")\n",
    "retriever.invoke(\"What is Bias Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af0a21",
   "metadata": {},
   "source": [
    "#### Define Custom Question answer Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3369282e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bias in LLMs refers to discriminatory tendencies in responses based on group attributes like race, gender, or ability. Tools like BiasAsker identify these biases by generating questions and analyzing system outputs. It highlights systemic disparities in conversational systems' behavior across different groups.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "system_prompt = (\n",
    "    \"You are an assistance for question-answering task\"\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "    \"Use three sentences maximum to answer the question and keep the answer concise.\"\n",
    "    \"The answer should be in markdown format.\"\n",
    "    \"\\n\\n{context}\\n\\n\"\n",
    ")\n",
    "def format_doc(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_doc,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    } \n",
    "    | prompt_template \n",
    "    | qwen_llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"What is Bias in LLM?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a19c4a",
   "metadata": {},
   "source": [
    "#### Define a Tool for Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6b0af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "@tool\n",
    "def bias_detection(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect Bias in the given statement and summarize the findings for me.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query related to bias in LLM\n",
    "    \n",
    "    Returns:\n",
    "        A string containing the summary for the bias-related finding from the query\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    context_str = (\"\\n\".join([doc.page_content for doc in retrieved_docs]))\n",
    "    bias_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are an expert bias analyst. Using the context below, identify biases.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Statement to analyze: {query}\n",
    "    \n",
    "    Summarize findings in exactly three bullet points.\n",
    "    \"\"\")\n",
    "    chain = bias_prompt | llama_llm\n",
    "    response = chain.invoke({\"context\": context_str, \"query\": query})\n",
    "    return response.content if hasattr(response, 'content') else str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969fd0b",
   "metadata": {},
   "source": [
    "#### Create a AI Agent with Bias Detection Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "tools = [bias_detection]\n",
    "bias_detection_agent = create_agent(\n",
    "    model=qwen_llm,\n",
    "    tools=tools,\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195533ba",
   "metadata": {},
   "source": [
    "#### Invoke agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "152fdf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='67fc2a7d-8246-4173-b523-9c43c5b2c463')]}\n",
      "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:06.105278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16090263209, 'load_duration': 104779000, 'prompt_eval_count': 184, 'prompt_eval_duration': 2423819916, 'eval_count': 223, 'eval_duration': 13484196756, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-517b-7ee1-b029-ffe5ba9d5377-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '43f8a62e-1e21-4df6-8ed7-7b3f918b048f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='67fc2a7d-8246-4173-b523-9c43c5b2c463'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:06.105278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16090263209, 'load_duration': 104779000, 'prompt_eval_count': 184, 'prompt_eval_duration': 2423819916, 'eval_count': 223, 'eval_duration': 13484196756, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-517b-7ee1-b029-ffe5ba9d5377-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '43f8a62e-1e21-4df6-8ed7-7b3f918b048f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407})]}\n",
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content=\"content='Here are three bullet points summarizing the bias-related points:\\\\n\\\\n• The author introduces two evaluation frameworks to measure social bias (BiasAsker) and cultural bias (XCulturalBench) in LLMs, highlighting the need for fairness assessment in conversational AI systems.\\\\n\\\\n• Experiments show that BiasAsker can identify bias altitudes on 841 groups from 5,021 biased properties perspective, demonstrating its effectiveness in measuring social bias in commercial systems and models.\\\\n\\\\n• The author identifies a cultural dominance issue within LLMs due to the predominant use of English, which may lead to biased responses towards non-English speakers or cultures, as seen in the example where ChatGPT responds with a condescending attitude towards poor individuals.' additional_kwargs={} response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-12-19T17:16:13.839031Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7221005833, 'load_duration': 104982083, 'prompt_eval_count': 656, 'prompt_eval_duration': 1690676084, 'eval_count': 147, 'eval_duration': 4324413912, 'logprobs': None, 'model_name': 'llama3.2:latest', 'model_provider': 'ollama'} id='lc_run--019b379c-9257-7091-b838-806c363de050-0' usage_metadata={'input_tokens': 656, 'output_tokens': 147, 'total_tokens': 803}\", name='bias_detection', id='fa5a5c60-f4ca-4b91-96b2-236fd828bfdd', tool_call_id='43f8a62e-1e21-4df6-8ed7-7b3f918b048f')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='67fc2a7d-8246-4173-b523-9c43c5b2c463'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:06.105278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16090263209, 'load_duration': 104779000, 'prompt_eval_count': 184, 'prompt_eval_duration': 2423819916, 'eval_count': 223, 'eval_duration': 13484196756, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-517b-7ee1-b029-ffe5ba9d5377-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '43f8a62e-1e21-4df6-8ed7-7b3f918b048f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407}), ToolMessage(content=\"content='Here are three bullet points summarizing the bias-related points:\\\\n\\\\n• The author introduces two evaluation frameworks to measure social bias (BiasAsker) and cultural bias (XCulturalBench) in LLMs, highlighting the need for fairness assessment in conversational AI systems.\\\\n\\\\n• Experiments show that BiasAsker can identify bias altitudes on 841 groups from 5,021 biased properties perspective, demonstrating its effectiveness in measuring social bias in commercial systems and models.\\\\n\\\\n• The author identifies a cultural dominance issue within LLMs due to the predominant use of English, which may lead to biased responses towards non-English speakers or cultures, as seen in the example where ChatGPT responds with a condescending attitude towards poor individuals.' additional_kwargs={} response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-12-19T17:16:13.839031Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7221005833, 'load_duration': 104982083, 'prompt_eval_count': 656, 'prompt_eval_duration': 1690676084, 'eval_count': 147, 'eval_duration': 4324413912, 'logprobs': None, 'model_name': 'llama3.2:latest', 'model_provider': 'ollama'} id='lc_run--019b379c-9257-7091-b838-806c363de050-0' usage_metadata={'input_tokens': 656, 'output_tokens': 147, 'total_tokens': 803}\", name='bias_detection', id='fa5a5c60-f4ca-4b91-96b2-236fd828bfdd', tool_call_id='43f8a62e-1e21-4df6-8ed7-7b3f918b048f')]}\n",
      "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content='The provided analysis highlights systemic biases in large language models (LLMs) rather than directly addressing the specific statement \"He is amazing Asian man.\" Here\\'s a concise summary of the key findings from the tool\\'s response:\\n\\n1. **Social Bias Measurement**: Frameworks like *BiasAsker* are designed to detect biases across 841 groups and 5,021 properties, revealing how LLMs may perpetuate stereotypes or unfair treatment of marginalized groups.\\n\\n2. **Cultural Dominance Issue**: LLMs often prioritize English-centric data, leading to biased responses toward non-English speakers or cultures. For example, models may exhibit condescending attitudes toward certain groups (e.g., poverty-related stereotypes).\\n\\n3. **Need for Fairness Assessments**: The study underscores the importance of evaluating LLMs for fairness, especially in conversational AI, to mitigate cultural and social biases embedded in training data.\\n\\nYour original statement (\"He is amazing Asian man\") appears to be a subjective opinion, but the tool\\'s findings focus on broader systemic issues in AI models. If you meant to analyze the statement for bias, further clarification would be needed—e.g., whether the phrase reflects stereotypes or cultural assumptions. Let me know if you\\'d like to explore this further!', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:49.13263Z', 'done': True, 'done_reason': 'stop', 'total_duration': 35280981917, 'load_duration': 61589375, 'prompt_eval_count': 621, 'prompt_eval_duration': 3127822042, 'eval_count': 524, 'eval_duration': 31932908106, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-ae99-7402-a91f-43fa611a2899-0', usage_metadata={'input_tokens': 621, 'output_tokens': 524, 'total_tokens': 1145})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='67fc2a7d-8246-4173-b523-9c43c5b2c463'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:06.105278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16090263209, 'load_duration': 104779000, 'prompt_eval_count': 184, 'prompt_eval_duration': 2423819916, 'eval_count': 223, 'eval_duration': 13484196756, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-517b-7ee1-b029-ffe5ba9d5377-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '43f8a62e-1e21-4df6-8ed7-7b3f918b048f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407}), ToolMessage(content=\"content='Here are three bullet points summarizing the bias-related points:\\\\n\\\\n• The author introduces two evaluation frameworks to measure social bias (BiasAsker) and cultural bias (XCulturalBench) in LLMs, highlighting the need for fairness assessment in conversational AI systems.\\\\n\\\\n• Experiments show that BiasAsker can identify bias altitudes on 841 groups from 5,021 biased properties perspective, demonstrating its effectiveness in measuring social bias in commercial systems and models.\\\\n\\\\n• The author identifies a cultural dominance issue within LLMs due to the predominant use of English, which may lead to biased responses towards non-English speakers or cultures, as seen in the example where ChatGPT responds with a condescending attitude towards poor individuals.' additional_kwargs={} response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-12-19T17:16:13.839031Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7221005833, 'load_duration': 104982083, 'prompt_eval_count': 656, 'prompt_eval_duration': 1690676084, 'eval_count': 147, 'eval_duration': 4324413912, 'logprobs': None, 'model_name': 'llama3.2:latest', 'model_provider': 'ollama'} id='lc_run--019b379c-9257-7091-b838-806c363de050-0' usage_metadata={'input_tokens': 656, 'output_tokens': 147, 'total_tokens': 803}\", name='bias_detection', id='fa5a5c60-f4ca-4b91-96b2-236fd828bfdd', tool_call_id='43f8a62e-1e21-4df6-8ed7-7b3f918b048f'), AIMessage(content='The provided analysis highlights systemic biases in large language models (LLMs) rather than directly addressing the specific statement \"He is amazing Asian man.\" Here\\'s a concise summary of the key findings from the tool\\'s response:\\n\\n1. **Social Bias Measurement**: Frameworks like *BiasAsker* are designed to detect biases across 841 groups and 5,021 properties, revealing how LLMs may perpetuate stereotypes or unfair treatment of marginalized groups.\\n\\n2. **Cultural Dominance Issue**: LLMs often prioritize English-centric data, leading to biased responses toward non-English speakers or cultures. For example, models may exhibit condescending attitudes toward certain groups (e.g., poverty-related stereotypes).\\n\\n3. **Need for Fairness Assessments**: The study underscores the importance of evaluating LLMs for fairness, especially in conversational AI, to mitigate cultural and social biases embedded in training data.\\n\\nYour original statement (\"He is amazing Asian man\") appears to be a subjective opinion, but the tool\\'s findings focus on broader systemic issues in AI models. If you meant to analyze the statement for bias, further clarification would be needed—e.g., whether the phrase reflects stereotypes or cultural assumptions. Let me know if you\\'d like to explore this further!', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:49.13263Z', 'done': True, 'done_reason': 'stop', 'total_duration': 35280981917, 'load_duration': 61589375, 'prompt_eval_count': 621, 'prompt_eval_duration': 3127822042, 'eval_count': 524, 'eval_duration': 31932908106, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-ae99-7402-a91f-43fa611a2899-0', usage_metadata={'input_tokens': 621, 'output_tokens': 524, 'total_tokens': 1145})]}\n",
      "The provided analysis highlights systemic biases in large language models (LLMs) rather than directly addressing the specific statement \"He is amazing Asian man.\" Here's a concise summary of the key findings from the tool's response:\n",
      "\n",
      "1. **Social Bias Measurement**: Frameworks like *BiasAsker* are designed to detect biases across 841 groups and 5,021 properties, revealing how LLMs may perpetuate stereotypes or unfair treatment of marginalized groups.\n",
      "\n",
      "2. **Cultural Dominance Issue**: LLMs often prioritize English-centric data, leading to biased responses toward non-English speakers or cultures. For example, models may exhibit condescending attitudes toward certain groups (e.g., poverty-related stereotypes).\n",
      "\n",
      "3. **Need for Fairness Assessments**: The study underscores the importance of evaluating LLMs for fairness, especially in conversational AI, to mitigate cultural and social biases embedded in training data.\n",
      "\n",
      "Your original statement (\"He is amazing Asian man\") appears to be a subjective opinion, but the tool's findings focus on broader systemic issues in AI models. If you meant to analyze the statement for bias, further clarification would be needed—e.g., whether the phrase reflects stereotypes or cultural assumptions. Let me know if you'd like to explore this further!\n"
     ]
    }
   ],
   "source": [
    "response = bias_detection_agent.invoke({'messages': \"He is amazing asian man\"})\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dd37671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is amazing asian man\n",
      "bias_detection\n",
      "{'query': 'He is amazing asian man'}\n",
      "content='Here are three bullet points summarizing the bias-related points:\\n\\n• The author introduces two evaluation frameworks to measure social bias (BiasAsker) and cultural bias (XCulturalBench) in LLMs, highlighting the need for fairness assessment in conversational AI systems.\\n\\n• Experiments show that BiasAsker can identify bias altitudes on 841 groups from 5,021 biased properties perspective, demonstrating its effectiveness in measuring social bias in commercial systems and models.\\n\\n• The author identifies a cultural dominance issue within LLMs due to the predominant use of English, which may lead to biased responses towards non-English speakers or cultures, as seen in the example where ChatGPT responds with a condescending attitude towards poor individuals.' additional_kwargs={} response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-12-19T17:16:13.839031Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7221005833, 'load_duration': 104982083, 'prompt_eval_count': 656, 'prompt_eval_duration': 1690676084, 'eval_count': 147, 'eval_duration': 4324413912, 'logprobs': None, 'model_name': 'llama3.2:latest', 'model_provider': 'ollama'} id='lc_run--019b379c-9257-7091-b838-806c363de050-0' usage_metadata={'input_tokens': 656, 'output_tokens': 147, 'total_tokens': 803}\n",
      "The provided analysis highlights systemic biases in large language models (LLMs) rather than directly addressing the specific statement \"He is amazing Asian man.\" Here's a concise summary of the key findings from the tool's response:\n",
      "\n",
      "1. **Social Bias Measurement**: Frameworks like *BiasAsker* are designed to detect biases across 841 groups and 5,021 properties, revealing how LLMs may perpetuate stereotypes or unfair treatment of marginalized groups.\n",
      "\n",
      "2. **Cultural Dominance Issue**: LLMs often prioritize English-centric data, leading to biased responses toward non-English speakers or cultures. For example, models may exhibit condescending attitudes toward certain groups (e.g., poverty-related stereotypes).\n",
      "\n",
      "3. **Need for Fairness Assessments**: The study underscores the importance of evaluating LLMs for fairness, especially in conversational AI, to mitigate cultural and social biases embedded in training data.\n",
      "\n",
      "Your original statement (\"He is amazing Asian man\") appears to be a subjective opinion, but the tool's findings focus on broader systemic issues in AI models. If you meant to analyze the statement for bias, further clarification would be needed—e.g., whether the phrase reflects stereotypes or cultural assumptions. Let me know if you'd like to explore this further!\n"
     ]
    }
   ],
   "source": [
    "for blocks in response[\"messages\"]:\n",
    "    for block in blocks.content_blocks:\n",
    "        if block['type'] == 'tool_call':\n",
    "            print(block[\"name\"])\n",
    "            print(block[\"args\"])\n",
    "        if block['type'] == 'text':\n",
    "            print(block[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae987fb5",
   "metadata": {},
   "source": [
    "#### Create Evaluation Data Set For Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"./TestDataSet/dataset.csv\")\n",
    "dataset = []\n",
    "\n",
    "for query, reference in zip(test_df[\"query\"], test_df[\"answer\"]):\n",
    "    retrieved_result = retriever.invoke(query)\n",
    "    relevant_docs = [doc.page_content for doc in retrieved_result]\n",
    "    agent_output = bias_detection_agent.invoke({\"messages\": [(\"user\", query)]})\n",
    "    response = agent_output[\"messages\"][-1].content\n",
    "    dataset.append({\n",
    "        \"user_input\": query,\n",
    "        \"retrieved_context\": relevant_docs,\n",
    "        \"response\": response,\n",
    "        \"reference\": reference\n",
    "    })\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33076e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_context</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He is amazing asian man, is there any bias ?</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The statement \"He is amazing asian man\" appear...</td>\n",
       "      <td>There does not appear to be any explicit bias ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mens are always strong, is there any bias ?</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The provided response from the bias detection ...</td>\n",
       "      <td>The statement 'Mens are always strong' exhibit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The real culprit in the war was hamas, not isr...</td>\n",
       "      <td>[Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4....</td>\n",
       "      <td>The tool's response provides general informati...</td>\n",
       "      <td>The query provided does not directly relate to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boys school got better mark than girls school ...</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The tool's response addresses biases in AI sys...</td>\n",
       "      <td>The query suggests potential bias but does not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Low class people are the reason for all the cr...</td>\n",
       "      <td>[Conversely, when a clause assumes the form of...</td>\n",
       "      <td>The statement \"Low class people are the reason...</td>\n",
       "      <td>The statement 'Low class people are the reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As he is asian, his accent is bad in english, ...</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The statement \"As he is Asian, his accent is b...</td>\n",
       "      <td>The statement 'As he is asian, his accent is b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0       He is amazing asian man, is there any bias ?   \n",
       "1        Mens are always strong, is there any bias ?   \n",
       "2  The real culprit in the war was hamas, not isr...   \n",
       "3  Boys school got better mark than girls school ...   \n",
       "4  Low class people are the reason for all the cr...   \n",
       "5  As he is asian, his accent is bad in english, ...   \n",
       "\n",
       "                                   retrieved_context  \\\n",
       "0  [without considering the differences across la...   \n",
       "1  [without considering the differences across la...   \n",
       "2  [Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4....   \n",
       "3  [without considering the differences across la...   \n",
       "4  [Conversely, when a clause assumes the form of...   \n",
       "5  [without considering the differences across la...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The statement \"He is amazing asian man\" appear...   \n",
       "1  The provided response from the bias detection ...   \n",
       "2  The tool's response provides general informati...   \n",
       "3  The tool's response addresses biases in AI sys...   \n",
       "4  The statement \"Low class people are the reason...   \n",
       "5  The statement \"As he is Asian, his accent is b...   \n",
       "\n",
       "                                           reference  \n",
       "0  There does not appear to be any explicit bias ...  \n",
       "1  The statement 'Mens are always strong' exhibit...  \n",
       "2  The query provided does not directly relate to...  \n",
       "3  The query suggests potential bias but does not...  \n",
       "4  The statement 'Low class people are the reason...  \n",
       "5  The statement 'As he is asian, his accent is b...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b10b5",
   "metadata": {},
   "source": [
    "#### Evaluation using RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "849802a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6/6 [00:59<00:00,  9.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factual_correctness(mode=f1)': 0.4783}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import FactualCorrectness\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas import EvaluationDataset\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "cloud_llm = ChatOllama(base_url=\"https://ollama.com\",\n",
    "                        model=\"gpt-oss:120b-cloud\",\n",
    "                        temperature=0,\n",
    "                        max_tokens=100)\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "eval_results = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[FactualCorrectness()],\n",
    "    llm=cloud_llm,\n",
    "    run_config=RunConfig(max_workers=1, timeout=600)\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12f64380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He is amazing asian man, is there any bias ?</td>\n",
       "      <td>The statement \"He is amazing asian man\" appear...</td>\n",
       "      <td>There does not appear to be any explicit bias ...</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mens are always strong, is there any bias ?</td>\n",
       "      <td>The provided response from the bias detection ...</td>\n",
       "      <td>The statement 'Mens are always strong' exhibit...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The real culprit in the war was hamas, not isr...</td>\n",
       "      <td>The tool's response provides general informati...</td>\n",
       "      <td>The query provided does not directly relate to...</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boys school got better mark than girls school ...</td>\n",
       "      <td>The tool's response addresses biases in AI sys...</td>\n",
       "      <td>The query suggests potential bias but does not...</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Low class people are the reason for all the cr...</td>\n",
       "      <td>The statement \"Low class people are the reason...</td>\n",
       "      <td>The statement 'Low class people are the reason...</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As he is asian, his accent is bad in english, ...</td>\n",
       "      <td>The statement \"As he is Asian, his accent is b...</td>\n",
       "      <td>The statement 'As he is asian, his accent is b...</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0       He is amazing asian man, is there any bias ?   \n",
       "1        Mens are always strong, is there any bias ?   \n",
       "2  The real culprit in the war was hamas, not isr...   \n",
       "3  Boys school got better mark than girls school ...   \n",
       "4  Low class people are the reason for all the cr...   \n",
       "5  As he is asian, his accent is bad in english, ...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The statement \"He is amazing asian man\" appear...   \n",
       "1  The provided response from the bias detection ...   \n",
       "2  The tool's response provides general informati...   \n",
       "3  The tool's response addresses biases in AI sys...   \n",
       "4  The statement \"Low class people are the reason...   \n",
       "5  The statement \"As he is Asian, his accent is b...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  There does not appear to be any explicit bias ...   \n",
       "1  The statement 'Mens are always strong' exhibit...   \n",
       "2  The query provided does not directly relate to...   \n",
       "3  The query suggests potential bias but does not...   \n",
       "4  The statement 'Low class people are the reason...   \n",
       "5  The statement 'As he is asian, his accent is b...   \n",
       "\n",
       "   factual_correctness(mode=f1)  \n",
       "0                          0.80  \n",
       "1                          0.00  \n",
       "2                          0.40  \n",
       "3                          0.60  \n",
       "4                          0.64  \n",
       "5                          0.43  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
