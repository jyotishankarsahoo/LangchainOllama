{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a77897c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc66ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "qwen_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                      model=\"qwen3:8b\",\n",
    "                      temperature=0,\n",
    "                      max_tokens = 250)\n",
    "llama_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                      model=\"llama3.2:latest\",\n",
    "                      temperature=0,\n",
    "                      max_tokens = 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5d3ad",
   "metadata": {},
   "source": [
    "#### Define Vector Store from persistent Storage and Retriever  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a316aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b208cb73-c426-4bec-a2bd-65a5daea3565', metadata={'total_pages': 15, 'moddate': '2025-01-07T01:36:50+00:00', 'subject': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'producer': 'pdfTeX-1.40.25', 'trapped': '/False', 'keywords': '', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-07T01:36:50+00:00', 'page': 9, 'page_label': '10', 'start_index': 0, 'source': './SampleData/LLMForgetting.pdf', 'author': '', 'title': ''}, page_content='Under review\\nFigure 6: The performance of general knowledge of the BLOOMZ-7.1b and LLAMA-7b\\nmodel trained on the instruction data and the mixed data. The dashed lines refers to the\\nperformance of BLOOMZ-7.1b and LLAMA-7B and the solid ones refer to those of mixed-\\ninstruction trained models.\\nincreases to 3b, BLOOMZ-3b suffers less forgetting compared to mT0-3.7B. For example, the\\nFG value of BLOOMZ-3b is 11.09 which is 5.64 lower than that of mT0-3.7b. These results\\nsuggest that BLOOMZ, which has a decoder-only model architecture, can maintain more\\nknowledge during continual instruction tuning. This difference may be attributed to the\\nautoregressive nature of the model or the differences in training objectives. Furthermore,\\nthe results imply that as the model scale increases, decoder-only models may suffer from\\nless catastrophic forgetting compared to encoder-decoder models. As we observe, the\\nknowledge degraded more drastically in mT0.\\n5.4 Effect of General Instruction Tuning'),\n",
       " Document(id='26d1a778-f6d2-4438-a671-941f7df0130c', metadata={'author': '', 'keywords': '', 'creationdate': '2024-09-04T00:37:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'start_index': 0, 'page_label': '144', 'moddate': '2024-09-04T00:37:21+00:00', 'subject': '', 'page': 162, 'producer': 'pdfTeX-1.40.25', 'total_pages': 223, 'creator': 'LaTeX with hyperref', 'trapped': '/False', 'title': '', 'source': './SampleData/TestingAndEvaluatingLLM.pdf'}, page_content='144 CHAPTER 7. SOCIAL BIAS\\nThe third threat lies in the conversational AI systems used in\\nthe evaluation. We do not evaluate the performance of BiasAsker on\\nother systems. To mitigate this threat, we chose to test commercial\\nconversational systems and SOTA academic models provided by big\\ncompanies. In the future, we could test more commercial software and\\nresearch models to further mitigate this threat.\\n7.4.2 Conclusion\\nIn this chapter, we design and implement BiasAsker, the first au-\\ntomated testing framework for comprehensively measuring the social\\nbiases in conversational AI systems. BiasAsker is able to evaluate 1) to\\nwhat degree is the system biased and 2) how social groups and biased\\nproperties are associated in the system. We conduct experiments\\non eight widely deployed commercial conversational AI systems and\\ntwo famous research models and demonstrate that BiasAsker can\\neffectively trigger a massive amount of biased behavior.\\n7.4.3 Limitations'),\n",
       " Document(id='5209807c-a7cc-4303-8896-f609fea10222', metadata={'trapped': '/False', 'creationdate': '2024-09-04T00:37:21+00:00', 'source': './SampleData/TestingAndEvaluatingLLM.pdf', 'page_label': '60', 'creator': 'LaTeX with hyperref', 'start_index': 0, 'title': '', 'author': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page': 78, 'total_pages': 223, 'moddate': '2024-09-04T00:37:21+00:00', 'keywords': '', 'producer': 'pdfTeX-1.40.25', 'subject': ''}, page_content='60 CHAPTER 4. LOGICAL REASONING CORRECTNESS\\nthe following challenges: 1) If an LLM concludes correctly, it is unclear\\nwhether the response stems from reasoning or merely relies on simple\\nheuristics such as memorization or word correlations (e.g., “dry floor”\\nis more likely to correlate with “playing football”). 2) If an LLM\\nfails to reason correctly, it is not clear which part of the reasoning\\nprocess it failed (i.e., inferring not raining from floor being dry or\\ninferring playing football from not raining). 3) There is a lack of\\na system that can organize such test cases to cover all other formal\\nreasoning scenarios besides implication, such as logical equivalence\\n(e.g., If A then B, if B then A; therefore, A if and only if B). 4)\\nFurthermore, understanding an LLM’s performance on such test cases\\nprovides little guidance on improving the reasoning ability of the\\nLLM. To better handle these challenges, a well-performing testing')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "ollama_embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vector_store = Chroma(persist_directory=\"./../RagAgent/chroma_langchain_db\",\n",
    "                      embedding_function=ollama_embedding)\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\": 3}\n",
    ")\n",
    "retriever.invoke(\"What is Bias Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af0a21",
   "metadata": {},
   "source": [
    "#### Define Custom Question answer Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3369282e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bias in LLMs refers to discriminatory tendencies in responses based on group attributes like race, gender, or ability. Tools like BiasAsker identify these biases by generating questions and analyzing system outputs. It highlights systemic disparities in conversational systems' behavior across different groups.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "system_prompt = (\n",
    "    \"You are an assistance for question-answering task\"\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "    \"Use three sentences maximum to answer the question and keep the answer concise.\"\n",
    "    \"The answer should be in markdown format.\"\n",
    "    \"\\n\\n{context}\\n\\n\"\n",
    ")\n",
    "def format_doc(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_doc,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    } \n",
    "    | prompt_template \n",
    "    | qwen_llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"What is Bias in LLM?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a19c4a",
   "metadata": {},
   "source": [
    "#### Define a Tool for Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "@tool\n",
    "def bias_detection(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect Bias in the given statement and summarize the findings for me.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query related to bias in LLM\n",
    "    \n",
    "    Returns:\n",
    "        A string containing the summary for the bias-related finding from the query\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    context_str = (\"\\n\".join([doc.page_content for doc in retrieved_docs]))\n",
    "    prompt = f\"\"\"\n",
    "    The following text discusses potential bias in LLM:\n",
    "    \n",
    "    {context_str}\n",
    "    \n",
    "    Please extract and summarize the bias-related points in exactly **Three bullet points.\n",
    "    \"\"\"\n",
    "    response = llama_llm.invoke(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969fd0b",
   "metadata": {},
   "source": [
    "#### Create a AI Agent with Bias Detection Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94bbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "tools = [bias_detection]\n",
    "bias_detection_agent = create_agent(\n",
    "    model=qwen_llm,\n",
    "    tools=tools,\n",
    "    debug=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195533ba",
   "metadata": {},
   "source": [
    "#### Invoke agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "152fdf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='a84e95a4-384d-4ac7-9617-ef82cefff4bd')]}\n",
      "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T16:40:16.083541Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14946751334, 'load_duration': 99117167, 'prompt_eval_count': 184, 'prompt_eval_duration': 1501424417, 'eval_count': 223, 'eval_duration': 13278475871, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b377b-876d-75d0-888d-52f1f6361bcf-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '1a32a944-1180-4052-ae49-f35033b1f0fb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='a84e95a4-384d-4ac7-9617-ef82cefff4bd'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T16:40:16.083541Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14946751334, 'load_duration': 99117167, 'prompt_eval_count': 184, 'prompt_eval_duration': 1501424417, 'eval_count': 223, 'eval_duration': 13278475871, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b377b-876d-75d0-888d-52f1f6361bcf-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '1a32a944-1180-4052-ae49-f35033b1f0fb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407})]}\n",
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content=\"content='- **Social bias measurement**: BiasAsker identifies and quantifies social biases (e.g., race, gender, religion) across 841 groups via targeted questions, revealing disparities in 10 commercial LLMs.  \\\\n- **Cultural dominance bias**: LLMs exhibit cultural bias due to overrepresentation of dominant languages/data, leading to skewed responses (e.g., English vs. Chinese prompts in the table).  \\\\n- **Language-specific bias**: The Chinese response to a prompt demonstrates overtly biased behavior (e.g., dehumanizing poor individuals), contrasting with the English response’s ethical rejection of harmful content.' additional_kwargs={} response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T16:40:46.57279Z', 'done': True, 'done_reason': 'stop', 'total_duration': 29526421041, 'load_duration': 76388791, 'prompt_eval_count': 652, 'prompt_eval_duration': 3791966833, 'eval_count': 419, 'eval_duration': 25537269212, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'} id='lc_run--019b377b-c593-7a81-a5dd-aa0aecb4da79-0' usage_metadata={'input_tokens': 652, 'output_tokens': 419, 'total_tokens': 1071}\", name='bias_detection', id='3ad02c19-e03b-4a87-bcc6-afb11a33c2d9', tool_call_id='1a32a944-1180-4052-ae49-f35033b1f0fb')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='a84e95a4-384d-4ac7-9617-ef82cefff4bd'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T16:40:16.083541Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14946751334, 'load_duration': 99117167, 'prompt_eval_count': 184, 'prompt_eval_duration': 1501424417, 'eval_count': 223, 'eval_duration': 13278475871, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b377b-876d-75d0-888d-52f1f6361bcf-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '1a32a944-1180-4052-ae49-f35033b1f0fb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407}), ToolMessage(content=\"content='- **Social bias measurement**: BiasAsker identifies and quantifies social biases (e.g., race, gender, religion) across 841 groups via targeted questions, revealing disparities in 10 commercial LLMs.  \\\\n- **Cultural dominance bias**: LLMs exhibit cultural bias due to overrepresentation of dominant languages/data, leading to skewed responses (e.g., English vs. Chinese prompts in the table).  \\\\n- **Language-specific bias**: The Chinese response to a prompt demonstrates overtly biased behavior (e.g., dehumanizing poor individuals), contrasting with the English response’s ethical rejection of harmful content.' additional_kwargs={} response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T16:40:46.57279Z', 'done': True, 'done_reason': 'stop', 'total_duration': 29526421041, 'load_duration': 76388791, 'prompt_eval_count': 652, 'prompt_eval_duration': 3791966833, 'eval_count': 419, 'eval_duration': 25537269212, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'} id='lc_run--019b377b-c593-7a81-a5dd-aa0aecb4da79-0' usage_metadata={'input_tokens': 652, 'output_tokens': 419, 'total_tokens': 1071}\", name='bias_detection', id='3ad02c19-e03b-4a87-bcc6-afb11a33c2d9', tool_call_id='1a32a944-1180-4052-ae49-f35033b1f0fb')]}\n",
      "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content='The analysis of the statement \"He is amazing asian man\" reveals potential biases in how language and cultural contexts shape perceptions:\\n\\n1. **Social Bias Measurement**: The statement may reflect stereotypes about Asian men, such as associating them with specific traits (e.g., \"amazing\"). BiasAsker\\'s framework would quantify such biases by comparing how different groups (e.g., races, genders) are perceived across 841 demographic categories, highlighting disparities in language models\\' responses.\\n\\n2. **Cultural Dominance Bias**: The phrasing might inadvertently prioritize English-centric cultural norms, as LLMs often overrepresent dominant languages (e.g., English vs. Chinese). This could skew interpretations of \"amazing\" toward Western standards, marginalizing non-Western perspectives.\\n\\n3. **Language-Specific Bias**: If the statement were part of a prompt in Chinese, the model might generate responses with overt biases (e.g., dehumanizing language toward marginalized groups), contrasting with English prompts that reject harmful content. This suggests language-specific training data imbalances affect output.\\n\\n**Summary**: While the statement itself isn\\'t inherently biased, the analysis underscores how language models may perpetuate or amplify societal biases based on cultural dominance and training data. The phrasing could reinforce stereotypes if not critically examined.', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T16:41:31.55177Z', 'done': True, 'done_reason': 'stop', 'total_duration': 44965952667, 'load_duration': 64985917, 'prompt_eval_count': 588, 'prompt_eval_duration': 3439176333, 'eval_count': 659, 'eval_duration': 41256154465, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b377c-38f8-7c02-b5eb-2e07a131d491-0', usage_metadata={'input_tokens': 588, 'output_tokens': 659, 'total_tokens': 1247})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='a84e95a4-384d-4ac7-9617-ef82cefff4bd'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T16:40:16.083541Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14946751334, 'load_duration': 99117167, 'prompt_eval_count': 184, 'prompt_eval_duration': 1501424417, 'eval_count': 223, 'eval_duration': 13278475871, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b377b-876d-75d0-888d-52f1f6361bcf-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '1a32a944-1180-4052-ae49-f35033b1f0fb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407}), ToolMessage(content=\"content='- **Social bias measurement**: BiasAsker identifies and quantifies social biases (e.g., race, gender, religion) across 841 groups via targeted questions, revealing disparities in 10 commercial LLMs.  \\\\n- **Cultural dominance bias**: LLMs exhibit cultural bias due to overrepresentation of dominant languages/data, leading to skewed responses (e.g., English vs. Chinese prompts in the table).  \\\\n- **Language-specific bias**: The Chinese response to a prompt demonstrates overtly biased behavior (e.g., dehumanizing poor individuals), contrasting with the English response’s ethical rejection of harmful content.' additional_kwargs={} response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T16:40:46.57279Z', 'done': True, 'done_reason': 'stop', 'total_duration': 29526421041, 'load_duration': 76388791, 'prompt_eval_count': 652, 'prompt_eval_duration': 3791966833, 'eval_count': 419, 'eval_duration': 25537269212, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'} id='lc_run--019b377b-c593-7a81-a5dd-aa0aecb4da79-0' usage_metadata={'input_tokens': 652, 'output_tokens': 419, 'total_tokens': 1071}\", name='bias_detection', id='3ad02c19-e03b-4a87-bcc6-afb11a33c2d9', tool_call_id='1a32a944-1180-4052-ae49-f35033b1f0fb'), AIMessage(content='The analysis of the statement \"He is amazing asian man\" reveals potential biases in how language and cultural contexts shape perceptions:\\n\\n1. **Social Bias Measurement**: The statement may reflect stereotypes about Asian men, such as associating them with specific traits (e.g., \"amazing\"). BiasAsker\\'s framework would quantify such biases by comparing how different groups (e.g., races, genders) are perceived across 841 demographic categories, highlighting disparities in language models\\' responses.\\n\\n2. **Cultural Dominance Bias**: The phrasing might inadvertently prioritize English-centric cultural norms, as LLMs often overrepresent dominant languages (e.g., English vs. Chinese). This could skew interpretations of \"amazing\" toward Western standards, marginalizing non-Western perspectives.\\n\\n3. **Language-Specific Bias**: If the statement were part of a prompt in Chinese, the model might generate responses with overt biases (e.g., dehumanizing language toward marginalized groups), contrasting with English prompts that reject harmful content. This suggests language-specific training data imbalances affect output.\\n\\n**Summary**: While the statement itself isn\\'t inherently biased, the analysis underscores how language models may perpetuate or amplify societal biases based on cultural dominance and training data. The phrasing could reinforce stereotypes if not critically examined.', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T16:41:31.55177Z', 'done': True, 'done_reason': 'stop', 'total_duration': 44965952667, 'load_duration': 64985917, 'prompt_eval_count': 588, 'prompt_eval_duration': 3439176333, 'eval_count': 659, 'eval_duration': 41256154465, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b377c-38f8-7c02-b5eb-2e07a131d491-0', usage_metadata={'input_tokens': 588, 'output_tokens': 659, 'total_tokens': 1247})]}\n",
      "The analysis of the statement \"He is amazing asian man\" reveals potential biases in how language and cultural contexts shape perceptions:\n",
      "\n",
      "1. **Social Bias Measurement**: The statement may reflect stereotypes about Asian men, such as associating them with specific traits (e.g., \"amazing\"). BiasAsker's framework would quantify such biases by comparing how different groups (e.g., races, genders) are perceived across 841 demographic categories, highlighting disparities in language models' responses.\n",
      "\n",
      "2. **Cultural Dominance Bias**: The phrasing might inadvertently prioritize English-centric cultural norms, as LLMs often overrepresent dominant languages (e.g., English vs. Chinese). This could skew interpretations of \"amazing\" toward Western standards, marginalizing non-Western perspectives.\n",
      "\n",
      "3. **Language-Specific Bias**: If the statement were part of a prompt in Chinese, the model might generate responses with overt biases (e.g., dehumanizing language toward marginalized groups), contrasting with English prompts that reject harmful content. This suggests language-specific training data imbalances affect output.\n",
      "\n",
      "**Summary**: While the statement itself isn't inherently biased, the analysis underscores how language models may perpetuate or amplify societal biases based on cultural dominance and training data. The phrasing could reinforce stereotypes if not critically examined.\n"
     ]
    }
   ],
   "source": [
    "response = bias_detection_agent.invoke({'messages': \"He is amazing asian man\"})\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd37671",
   "metadata": {},
   "outputs": [],
   "source": [
    "for blocks in response[\"messages\"]:\n",
    "    for block in blocks.content_blocks:\n",
    "        if block['type'] == 'tool_call':\n",
    "            print(block[\"name\"])\n",
    "            print(block[\"args\"])\n",
    "        if block['type'] == 'text':\n",
    "            print(block[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae987fb5",
   "metadata": {},
   "source": [
    "#### Create Evaluation Data Set For Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183d8bc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
