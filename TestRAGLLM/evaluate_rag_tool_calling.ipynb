{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a77897c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc66ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "qwen_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                      model=\"qwen3:8b\",\n",
    "                      temperature=0,\n",
    "                      max_tokens = 250)\n",
    "llama_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                      model=\"llama3.2:latest\",\n",
    "                      temperature=0,\n",
    "                      max_tokens = 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5d3ad",
   "metadata": {},
   "source": [
    "#### Define Vector Store from persistent Storage and Retriever  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a316aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b208cb73-c426-4bec-a2bd-65a5daea3565', metadata={'creationdate': '2025-01-07T01:36:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': '', 'source': './SampleData/LLMForgetting.pdf', 'creator': 'LaTeX with hyperref', 'page_label': '10', 'page': 9, 'keywords': '', 'subject': '', 'producer': 'pdfTeX-1.40.25', 'moddate': '2025-01-07T01:36:50+00:00', 'total_pages': 15, 'trapped': '/False', 'author': '', 'start_index': 0}, page_content='Under review\\nFigure 6: The performance of general knowledge of the BLOOMZ-7.1b and LLAMA-7b\\nmodel trained on the instruction data and the mixed data. The dashed lines refers to the\\nperformance of BLOOMZ-7.1b and LLAMA-7B and the solid ones refer to those of mixed-\\ninstruction trained models.\\nincreases to 3b, BLOOMZ-3b suffers less forgetting compared to mT0-3.7B. For example, the\\nFG value of BLOOMZ-3b is 11.09 which is 5.64 lower than that of mT0-3.7b. These results\\nsuggest that BLOOMZ, which has a decoder-only model architecture, can maintain more\\nknowledge during continual instruction tuning. This difference may be attributed to the\\nautoregressive nature of the model or the differences in training objectives. Furthermore,\\nthe results imply that as the model scale increases, decoder-only models may suffer from\\nless catastrophic forgetting compared to encoder-decoder models. As we observe, the\\nknowledge degraded more drastically in mT0.\\n5.4 Effect of General Instruction Tuning'),\n",
       " Document(id='26d1a778-f6d2-4438-a671-941f7df0130c', metadata={'start_index': 0, 'creationdate': '2024-09-04T00:37:21+00:00', 'moddate': '2024-09-04T00:37:21+00:00', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'page_label': '144', 'subject': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'keywords': '', 'source': './SampleData/TestingAndEvaluatingLLM.pdf', 'total_pages': 223, 'page': 162, 'author': '', 'trapped': '/False', 'title': ''}, page_content='144 CHAPTER 7. SOCIAL BIAS\\nThe third threat lies in the conversational AI systems used in\\nthe evaluation. We do not evaluate the performance of BiasAsker on\\nother systems. To mitigate this threat, we chose to test commercial\\nconversational systems and SOTA academic models provided by big\\ncompanies. In the future, we could test more commercial software and\\nresearch models to further mitigate this threat.\\n7.4.2 Conclusion\\nIn this chapter, we design and implement BiasAsker, the first au-\\ntomated testing framework for comprehensively measuring the social\\nbiases in conversational AI systems. BiasAsker is able to evaluate 1) to\\nwhat degree is the system biased and 2) how social groups and biased\\nproperties are associated in the system. We conduct experiments\\non eight widely deployed commercial conversational AI systems and\\ntwo famous research models and demonstrate that BiasAsker can\\neffectively trigger a massive amount of biased behavior.\\n7.4.3 Limitations'),\n",
       " Document(id='5209807c-a7cc-4303-8896-f609fea10222', metadata={'source': './SampleData/TestingAndEvaluatingLLM.pdf', 'page_label': '60', 'page': 78, 'start_index': 0, 'trapped': '/False', 'keywords': '', 'title': '', 'creationdate': '2024-09-04T00:37:21+00:00', 'moddate': '2024-09-04T00:37:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with hyperref', 'total_pages': 223, 'producer': 'pdfTeX-1.40.25', 'author': '', 'subject': ''}, page_content='60 CHAPTER 4. LOGICAL REASONING CORRECTNESS\\nthe following challenges: 1) If an LLM concludes correctly, it is unclear\\nwhether the response stems from reasoning or merely relies on simple\\nheuristics such as memorization or word correlations (e.g., “dry floor”\\nis more likely to correlate with “playing football”). 2) If an LLM\\nfails to reason correctly, it is not clear which part of the reasoning\\nprocess it failed (i.e., inferring not raining from floor being dry or\\ninferring playing football from not raining). 3) There is a lack of\\na system that can organize such test cases to cover all other formal\\nreasoning scenarios besides implication, such as logical equivalence\\n(e.g., If A then B, if B then A; therefore, A if and only if B). 4)\\nFurthermore, understanding an LLM’s performance on such test cases\\nprovides little guidance on improving the reasoning ability of the\\nLLM. To better handle these challenges, a well-performing testing')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "ollama_embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vector_store = Chroma(persist_directory=\"./../RagAgent/chroma_langchain_db\",\n",
    "                      embedding_function=ollama_embedding)\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\": 3}\n",
    ")\n",
    "retriever.invoke(\"What is Bias Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af0a21",
   "metadata": {},
   "source": [
    "#### Define Custom Question answer Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3369282e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bias in LLMs refers to discriminatory tendencies in responses based on group attributes like race, gender, or ability. Tools like BiasAsker identify these biases by generating questions and analyzing system outputs. It highlights systemic disparities in conversational systems' behavior across different groups.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "system_prompt = (\n",
    "    \"You are an assistance for question-answering task\"\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "    \"Use three sentences maximum to answer the question and keep the answer concise.\"\n",
    "    \"The answer should be in markdown format.\"\n",
    "    \"\\n\\n{context}\\n\\n\"\n",
    ")\n",
    "def format_doc(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_doc,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    } \n",
    "    | prompt_template \n",
    "    | qwen_llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"What is Bias in LLM?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a19c4a",
   "metadata": {},
   "source": [
    "#### Define a Tool for Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6b0af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "@tool\n",
    "def bias_detection(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect Bias in the given statement and summarize the findings for me.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query related to bias in LLM\n",
    "    \n",
    "    Returns:\n",
    "        A string containing the summary for the bias-related finding from the query\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    context_str = (\"\\n\".join([doc.page_content for doc in retrieved_docs]))\n",
    "    bias_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are an expert bias analyst. Using the context below, identify biases.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Statement to analyze: {query}\n",
    "    \n",
    "    Summarize findings in exactly three bullet points.\n",
    "    \"\"\")\n",
    "    chain = bias_prompt | llama_llm\n",
    "    response = chain.invoke({\"context\": context_str, \"query\": query})\n",
    "    return response.content if hasattr(response, 'content') else str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969fd0b",
   "metadata": {},
   "source": [
    "#### Create a AI Agent with Bias Detection Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "tools = [bias_detection]\n",
    "bias_detection_agent = create_agent(\n",
    "    model=qwen_llm,\n",
    "    tools=tools,\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195533ba",
   "metadata": {},
   "source": [
    "#### Invoke agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "152fdf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='67fc2a7d-8246-4173-b523-9c43c5b2c463')]}\n",
      "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:06.105278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16090263209, 'load_duration': 104779000, 'prompt_eval_count': 184, 'prompt_eval_duration': 2423819916, 'eval_count': 223, 'eval_duration': 13484196756, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-517b-7ee1-b029-ffe5ba9d5377-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '43f8a62e-1e21-4df6-8ed7-7b3f918b048f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='67fc2a7d-8246-4173-b523-9c43c5b2c463'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:06.105278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16090263209, 'load_duration': 104779000, 'prompt_eval_count': 184, 'prompt_eval_duration': 2423819916, 'eval_count': 223, 'eval_duration': 13484196756, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-517b-7ee1-b029-ffe5ba9d5377-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '43f8a62e-1e21-4df6-8ed7-7b3f918b048f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407})]}\n",
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content=\"content='Here are three bullet points summarizing the bias-related points:\\\\n\\\\n• The author introduces two evaluation frameworks to measure social bias (BiasAsker) and cultural bias (XCulturalBench) in LLMs, highlighting the need for fairness assessment in conversational AI systems.\\\\n\\\\n• Experiments show that BiasAsker can identify bias altitudes on 841 groups from 5,021 biased properties perspective, demonstrating its effectiveness in measuring social bias in commercial systems and models.\\\\n\\\\n• The author identifies a cultural dominance issue within LLMs due to the predominant use of English, which may lead to biased responses towards non-English speakers or cultures, as seen in the example where ChatGPT responds with a condescending attitude towards poor individuals.' additional_kwargs={} response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-12-19T17:16:13.839031Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7221005833, 'load_duration': 104982083, 'prompt_eval_count': 656, 'prompt_eval_duration': 1690676084, 'eval_count': 147, 'eval_duration': 4324413912, 'logprobs': None, 'model_name': 'llama3.2:latest', 'model_provider': 'ollama'} id='lc_run--019b379c-9257-7091-b838-806c363de050-0' usage_metadata={'input_tokens': 656, 'output_tokens': 147, 'total_tokens': 803}\", name='bias_detection', id='fa5a5c60-f4ca-4b91-96b2-236fd828bfdd', tool_call_id='43f8a62e-1e21-4df6-8ed7-7b3f918b048f')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='67fc2a7d-8246-4173-b523-9c43c5b2c463'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:06.105278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16090263209, 'load_duration': 104779000, 'prompt_eval_count': 184, 'prompt_eval_duration': 2423819916, 'eval_count': 223, 'eval_duration': 13484196756, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-517b-7ee1-b029-ffe5ba9d5377-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '43f8a62e-1e21-4df6-8ed7-7b3f918b048f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407}), ToolMessage(content=\"content='Here are three bullet points summarizing the bias-related points:\\\\n\\\\n• The author introduces two evaluation frameworks to measure social bias (BiasAsker) and cultural bias (XCulturalBench) in LLMs, highlighting the need for fairness assessment in conversational AI systems.\\\\n\\\\n• Experiments show that BiasAsker can identify bias altitudes on 841 groups from 5,021 biased properties perspective, demonstrating its effectiveness in measuring social bias in commercial systems and models.\\\\n\\\\n• The author identifies a cultural dominance issue within LLMs due to the predominant use of English, which may lead to biased responses towards non-English speakers or cultures, as seen in the example where ChatGPT responds with a condescending attitude towards poor individuals.' additional_kwargs={} response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-12-19T17:16:13.839031Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7221005833, 'load_duration': 104982083, 'prompt_eval_count': 656, 'prompt_eval_duration': 1690676084, 'eval_count': 147, 'eval_duration': 4324413912, 'logprobs': None, 'model_name': 'llama3.2:latest', 'model_provider': 'ollama'} id='lc_run--019b379c-9257-7091-b838-806c363de050-0' usage_metadata={'input_tokens': 656, 'output_tokens': 147, 'total_tokens': 803}\", name='bias_detection', id='fa5a5c60-f4ca-4b91-96b2-236fd828bfdd', tool_call_id='43f8a62e-1e21-4df6-8ed7-7b3f918b048f')]}\n",
      "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content='The provided analysis highlights systemic biases in large language models (LLMs) rather than directly addressing the specific statement \"He is amazing Asian man.\" Here\\'s a concise summary of the key findings from the tool\\'s response:\\n\\n1. **Social Bias Measurement**: Frameworks like *BiasAsker* are designed to detect biases across 841 groups and 5,021 properties, revealing how LLMs may perpetuate stereotypes or unfair treatment of marginalized groups.\\n\\n2. **Cultural Dominance Issue**: LLMs often prioritize English-centric data, leading to biased responses toward non-English speakers or cultures. For example, models may exhibit condescending attitudes toward certain groups (e.g., poverty-related stereotypes).\\n\\n3. **Need for Fairness Assessments**: The study underscores the importance of evaluating LLMs for fairness, especially in conversational AI, to mitigate cultural and social biases embedded in training data.\\n\\nYour original statement (\"He is amazing Asian man\") appears to be a subjective opinion, but the tool\\'s findings focus on broader systemic issues in AI models. If you meant to analyze the statement for bias, further clarification would be needed—e.g., whether the phrase reflects stereotypes or cultural assumptions. Let me know if you\\'d like to explore this further!', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:49.13263Z', 'done': True, 'done_reason': 'stop', 'total_duration': 35280981917, 'load_duration': 61589375, 'prompt_eval_count': 621, 'prompt_eval_duration': 3127822042, 'eval_count': 524, 'eval_duration': 31932908106, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-ae99-7402-a91f-43fa611a2899-0', usage_metadata={'input_tokens': 621, 'output_tokens': 524, 'total_tokens': 1145})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='He is amazing asian man', additional_kwargs={}, response_metadata={}, id='67fc2a7d-8246-4173-b523-9c43c5b2c463'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:06.105278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16090263209, 'load_duration': 104779000, 'prompt_eval_count': 184, 'prompt_eval_duration': 2423819916, 'eval_count': 223, 'eval_duration': 13484196756, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-517b-7ee1-b029-ffe5ba9d5377-0', tool_calls=[{'name': 'bias_detection', 'args': {'query': 'He is amazing asian man'}, 'id': '43f8a62e-1e21-4df6-8ed7-7b3f918b048f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 223, 'total_tokens': 407}), ToolMessage(content=\"content='Here are three bullet points summarizing the bias-related points:\\\\n\\\\n• The author introduces two evaluation frameworks to measure social bias (BiasAsker) and cultural bias (XCulturalBench) in LLMs, highlighting the need for fairness assessment in conversational AI systems.\\\\n\\\\n• Experiments show that BiasAsker can identify bias altitudes on 841 groups from 5,021 biased properties perspective, demonstrating its effectiveness in measuring social bias in commercial systems and models.\\\\n\\\\n• The author identifies a cultural dominance issue within LLMs due to the predominant use of English, which may lead to biased responses towards non-English speakers or cultures, as seen in the example where ChatGPT responds with a condescending attitude towards poor individuals.' additional_kwargs={} response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-12-19T17:16:13.839031Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7221005833, 'load_duration': 104982083, 'prompt_eval_count': 656, 'prompt_eval_duration': 1690676084, 'eval_count': 147, 'eval_duration': 4324413912, 'logprobs': None, 'model_name': 'llama3.2:latest', 'model_provider': 'ollama'} id='lc_run--019b379c-9257-7091-b838-806c363de050-0' usage_metadata={'input_tokens': 656, 'output_tokens': 147, 'total_tokens': 803}\", name='bias_detection', id='fa5a5c60-f4ca-4b91-96b2-236fd828bfdd', tool_call_id='43f8a62e-1e21-4df6-8ed7-7b3f918b048f'), AIMessage(content='The provided analysis highlights systemic biases in large language models (LLMs) rather than directly addressing the specific statement \"He is amazing Asian man.\" Here\\'s a concise summary of the key findings from the tool\\'s response:\\n\\n1. **Social Bias Measurement**: Frameworks like *BiasAsker* are designed to detect biases across 841 groups and 5,021 properties, revealing how LLMs may perpetuate stereotypes or unfair treatment of marginalized groups.\\n\\n2. **Cultural Dominance Issue**: LLMs often prioritize English-centric data, leading to biased responses toward non-English speakers or cultures. For example, models may exhibit condescending attitudes toward certain groups (e.g., poverty-related stereotypes).\\n\\n3. **Need for Fairness Assessments**: The study underscores the importance of evaluating LLMs for fairness, especially in conversational AI, to mitigate cultural and social biases embedded in training data.\\n\\nYour original statement (\"He is amazing Asian man\") appears to be a subjective opinion, but the tool\\'s findings focus on broader systemic issues in AI models. If you meant to analyze the statement for bias, further clarification would be needed—e.g., whether the phrase reflects stereotypes or cultural assumptions. Let me know if you\\'d like to explore this further!', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-19T17:16:49.13263Z', 'done': True, 'done_reason': 'stop', 'total_duration': 35280981917, 'load_duration': 61589375, 'prompt_eval_count': 621, 'prompt_eval_duration': 3127822042, 'eval_count': 524, 'eval_duration': 31932908106, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b379c-ae99-7402-a91f-43fa611a2899-0', usage_metadata={'input_tokens': 621, 'output_tokens': 524, 'total_tokens': 1145})]}\n",
      "The provided analysis highlights systemic biases in large language models (LLMs) rather than directly addressing the specific statement \"He is amazing Asian man.\" Here's a concise summary of the key findings from the tool's response:\n",
      "\n",
      "1. **Social Bias Measurement**: Frameworks like *BiasAsker* are designed to detect biases across 841 groups and 5,021 properties, revealing how LLMs may perpetuate stereotypes or unfair treatment of marginalized groups.\n",
      "\n",
      "2. **Cultural Dominance Issue**: LLMs often prioritize English-centric data, leading to biased responses toward non-English speakers or cultures. For example, models may exhibit condescending attitudes toward certain groups (e.g., poverty-related stereotypes).\n",
      "\n",
      "3. **Need for Fairness Assessments**: The study underscores the importance of evaluating LLMs for fairness, especially in conversational AI, to mitigate cultural and social biases embedded in training data.\n",
      "\n",
      "Your original statement (\"He is amazing Asian man\") appears to be a subjective opinion, but the tool's findings focus on broader systemic issues in AI models. If you meant to analyze the statement for bias, further clarification would be needed—e.g., whether the phrase reflects stereotypes or cultural assumptions. Let me know if you'd like to explore this further!\n"
     ]
    }
   ],
   "source": [
    "response = bias_detection_agent.invoke({'messages': \"He is amazing asian man\"})\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dd37671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is amazing asian man\n",
      "bias_detection\n",
      "{'query': 'He is amazing asian man'}\n",
      "content='Here are three bullet points summarizing the bias-related points:\\n\\n• The author introduces two evaluation frameworks to measure social bias (BiasAsker) and cultural bias (XCulturalBench) in LLMs, highlighting the need for fairness assessment in conversational AI systems.\\n\\n• Experiments show that BiasAsker can identify bias altitudes on 841 groups from 5,021 biased properties perspective, demonstrating its effectiveness in measuring social bias in commercial systems and models.\\n\\n• The author identifies a cultural dominance issue within LLMs due to the predominant use of English, which may lead to biased responses towards non-English speakers or cultures, as seen in the example where ChatGPT responds with a condescending attitude towards poor individuals.' additional_kwargs={} response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-12-19T17:16:13.839031Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7221005833, 'load_duration': 104982083, 'prompt_eval_count': 656, 'prompt_eval_duration': 1690676084, 'eval_count': 147, 'eval_duration': 4324413912, 'logprobs': None, 'model_name': 'llama3.2:latest', 'model_provider': 'ollama'} id='lc_run--019b379c-9257-7091-b838-806c363de050-0' usage_metadata={'input_tokens': 656, 'output_tokens': 147, 'total_tokens': 803}\n",
      "The provided analysis highlights systemic biases in large language models (LLMs) rather than directly addressing the specific statement \"He is amazing Asian man.\" Here's a concise summary of the key findings from the tool's response:\n",
      "\n",
      "1. **Social Bias Measurement**: Frameworks like *BiasAsker* are designed to detect biases across 841 groups and 5,021 properties, revealing how LLMs may perpetuate stereotypes or unfair treatment of marginalized groups.\n",
      "\n",
      "2. **Cultural Dominance Issue**: LLMs often prioritize English-centric data, leading to biased responses toward non-English speakers or cultures. For example, models may exhibit condescending attitudes toward certain groups (e.g., poverty-related stereotypes).\n",
      "\n",
      "3. **Need for Fairness Assessments**: The study underscores the importance of evaluating LLMs for fairness, especially in conversational AI, to mitigate cultural and social biases embedded in training data.\n",
      "\n",
      "Your original statement (\"He is amazing Asian man\") appears to be a subjective opinion, but the tool's findings focus on broader systemic issues in AI models. If you meant to analyze the statement for bias, further clarification would be needed—e.g., whether the phrase reflects stereotypes or cultural assumptions. Let me know if you'd like to explore this further!\n"
     ]
    }
   ],
   "source": [
    "for blocks in response[\"messages\"]:\n",
    "    for block in blocks.content_blocks:\n",
    "        if block['type'] == 'tool_call':\n",
    "            print(block[\"name\"])\n",
    "            print(block[\"args\"])\n",
    "        if block['type'] == 'text':\n",
    "            print(block[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae987fb5",
   "metadata": {},
   "source": [
    "#### Create Evaluation Data Set For Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"./TestDataSet/dataset.csv\")\n",
    "dataset = []\n",
    "def extract_tool_data(agent_output):\n",
    "    \"\"\"\n",
    "    Extracts tool calls from the agent's message history.\n",
    "    \"\"\"\n",
    "    # Look for any message that contains tool_calls\n",
    "    actual_tool_calls = []\n",
    "    for msg in agent_output.get(\"messages\", []):\n",
    "        if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "            for tc in msg.tool_calls:\n",
    "                actual_tool_calls.append({\n",
    "                    \"name\": tc[\"name\"],\n",
    "                    \"args\": tc[\"args\"]\n",
    "                })\n",
    "    return actual_tool_calls\n",
    "for query, reference in zip(test_df[\"query\"], test_df[\"answer\"]):\n",
    "    retrieved_result = retriever.invoke(query)\n",
    "    relevant_docs = [doc.page_content for doc in retrieved_result]\n",
    "    agent_output = bias_detection_agent.invoke({\"messages\": [(\"user\", query)]})\n",
    "    response = agent_output[\"messages\"][-1].content\n",
    "    dataset.append({\n",
    "        \"user_input\": query,\n",
    "        \"retrieved_context\": relevant_docs,\n",
    "        \"response\": response,\n",
    "        \"reference\": reference,\n",
    "        \"tool_calls\": extract_tool_data(agent_output),\n",
    "        \"reference_tool_calls\": [\n",
    "        {\n",
    "            \"name\": \"bias_detection\", \n",
    "            \"args\": {\"query\": query}\n",
    "        }\n",
    "    ]\n",
    "    })\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0c8b17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_input': 'He is amazing asian man, is there any bias ?',\n",
       "  'retrieved_context': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].',\n",
       "   'Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.',\n",
       "   'Conversely, when a clause assumes the form ofP(x), Q(x), or similar\\nstructures, the subject is represented byx, as it functions as a variable.\\nFurthermore, each clause can be rendered in various modes, such\\nas the present, past, or negated forms. Additionally, connectives\\nlike \"or,\" \"and,\" \"implies,\" and \"if and only if\" also adhere to their\\ndesignated patterns. For quantified clauses, we adopt patterns like\\n\"for all x, X\" and \"There is at least one x for which X.\" Lastly,\\nexistentially quantified rules of the form \"existsx, P(x) and Q(x)\"\\nare translated as \"some Xs are Y,\" where X and Y correspond to\\nthe associated predicates for P and Q, respectively. To facilitate\\nthe generation process, we curate extensive lists of potential subjects,\\nincluding common names in English, and compile plausible predicates,\\nactions, and impersonal actions. As an example, the clause “p →q”\\ncould be translated into natural language as “If John plays Tennis,\\nthen it will snow.”'],\n",
       "  'response': 'The statement \"He is amazing asian man\" appears neutral on its own, but bias detection tools highlight broader systemic issues in AI models. Here\\'s a summary of key findings:\\n\\n1. **Cultural Bias in Training Data**: Large language models (LLMs) like ChatGPT may amplify dominant cultural narratives due to their English-centric training data, which can perpetuate stereotypes about Asian communities.\\n\\n2. **Measured Bias Rates**: Commercial models such as GPT-3 and Jovi exhibit measurable bias (e.g., 25.03% and 32.82% bias rates), indicating systemic tendencies to favor certain narratives or assumptions.\\n\\n3. **Translation and Representation Biases**: Models may inadvertently reinforce stereotypes through translation processes, such as assuming subject/predicate structures that reflect cultural biases (e.g., generalizing traits to entire groups).\\n\\nWhile the specific statement doesn\\'t contain explicit bias, the tools emphasize that AI systems can inherit and propagate societal biases, particularly when dealing with culturally specific terms or groups. This underscores the importance of critical evaluation when interpreting AI-generated content about marginalized communities.',\n",
       "  'reference': \"There does not appear to be any explicit bias in the phrase 'He is amazing asian man'. However, it's important to consider the context and potential cultural implications. Stereotyping or generalizing based on race can lead to biases. It's always best to use inclusive language.\",\n",
       "  'tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'He is amazing asian man, is there any bias ?'}}],\n",
       "  'reference_tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'He is amazing asian man, is there any bias ?'}}]},\n",
       " {'user_input': 'Mens are always strong, is there any bias ?',\n",
       "  'retrieved_context': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].',\n",
       "   'Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.',\n",
       "   'Conversely, when a clause assumes the form ofP(x), Q(x), or similar\\nstructures, the subject is represented byx, as it functions as a variable.\\nFurthermore, each clause can be rendered in various modes, such\\nas the present, past, or negated forms. Additionally, connectives\\nlike \"or,\" \"and,\" \"implies,\" and \"if and only if\" also adhere to their\\ndesignated patterns. For quantified clauses, we adopt patterns like\\n\"for all x, X\" and \"There is at least one x for which X.\" Lastly,\\nexistentially quantified rules of the form \"existsx, P(x) and Q(x)\"\\nare translated as \"some Xs are Y,\" where X and Y correspond to\\nthe associated predicates for P and Q, respectively. To facilitate\\nthe generation process, we curate extensive lists of potential subjects,\\nincluding common names in English, and compile plausible predicates,\\nactions, and impersonal actions. As an example, the clause “p →q”\\ncould be translated into natural language as “If John plays Tennis,\\nthen it will snow.”'],\n",
       "  'response': 'The provided response from the bias detection tool does not directly address the statement \"Mens are always strong\" or analyze gender-related bias in this context. Instead, it references a study on social bias in large language models (LLMs), focusing on socio-economic stereotypes rather than gender assumptions. \\n\\nThis suggests the tool\\'s response may not align with the specific query about gender bias. If you\\'d like a detailed analysis of the statement \"Mens are always strong\" for gender bias, I can provide that directly without relying on the tool. Would you like me to proceed with that?',\n",
       "  'reference': \"The statement 'Mens are always strong' exhibits gender bias by generalizing a physical attribute to all men, which is not accurate or fair. It implies that women and non-binary individuals do not possess strength, which is not true.\",\n",
       "  'tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'Mens are always strong'}}],\n",
       "  'reference_tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'Mens are always strong, is there any bias ?'}}]},\n",
       " {'user_input': 'The real culprit in the war was hamas, not israel, is there any bias ?',\n",
       "  'retrieved_context': ['Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.',\n",
       "   'without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].',\n",
       "   'Conversely, when a clause assumes the form ofP(x), Q(x), or similar\\nstructures, the subject is represented byx, as it functions as a variable.\\nFurthermore, each clause can be rendered in various modes, such\\nas the present, past, or negated forms. Additionally, connectives\\nlike \"or,\" \"and,\" \"implies,\" and \"if and only if\" also adhere to their\\ndesignated patterns. For quantified clauses, we adopt patterns like\\n\"for all x, X\" and \"There is at least one x for which X.\" Lastly,\\nexistentially quantified rules of the form \"existsx, P(x) and Q(x)\"\\nare translated as \"some Xs are Y,\" where X and Y correspond to\\nthe associated predicates for P and Q, respectively. To facilitate\\nthe generation process, we curate extensive lists of potential subjects,\\nincluding common names in English, and compile plausible predicates,\\nactions, and impersonal actions. As an example, the clause “p →q”\\ncould be translated into natural language as “If John plays Tennis,\\nthen it will snow.”'],\n",
       "  'response': \"The tool's response provides general insights about potential biases in large language models (LLMs) like ChatGPT or GPT-3, but it does **not** directly analyze your specific statement about Hamas and Israel. Here's a concise summary of the tool's findings:\\n\\n1. **Cultural Bias in LLMs**: Models may amplify dominant cultural narratives due to their training data (e.g., English-centric datasets), which could influence how they frame conflicts or responsibilities.\\n2. **Explicit Bias Rates**: Some commercial models exhibit measurable bias (e.g., 25.03% for GPT-3), meaning they may favor certain perspectives in responses.\\n3. **Translation Biases**: Models might perpetuate biases through translation, such as assuming subject/predicate existence in quantified clauses, leading to culturally insensitive outputs.\\n\\n### Key Takeaway:\\nThe tool highlights **systemic biases in AI models**, not necessarily in your statement. If you'd like, I can help analyze your statement for potential biases in framing (e.g., selective focus, omission of context). Let me know how you'd like to proceed!\",\n",
       "  'reference': \"The query provided does not directly relate to bias in language models. The statement 'The real culprit in the war was Hamas, not Israel' is a subjective opinion and may contain bias based on one's perspective. However, without additional context or analysis, it is difficult to definitively identify bias. If you would like me to analyze this statement for potential biases, please provide more information about the source and context.\",\n",
       "  'tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'The real culprit in the war was Hamas, not Israel, is there any bias ?'}}],\n",
       "  'reference_tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'The real culprit in the war was hamas, not israel, is there any bias ?'}}]},\n",
       " {'user_input': 'Boys school got better mark than girls school this year, is there any bias ?',\n",
       "  'retrieved_context': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].',\n",
       "   'Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.',\n",
       "   '122 CHAPTER 7. SOCIAL BIAS\\nand record potential biases, then calculate the bias scores from the\\nperspective of relative bias and absolute bias, finally summarize and\\nvisualize the latent associations in chatbots under-test. In particular,\\nBiasAskercurrentlycantestconversationalAIsystemsinbothEnglish\\nand Chinese, two widely used languages over the world.\\nTo evaluate the performance of BiasAsker, we apply BiasAsker to\\ntest eight widely deployed commercial conversational AI systems and\\ntwo famous conversational research models from famous companies,\\nincluding OpenAI, Meta, Microsoft, Xiaomi, OPPO, Vivo, and\\nTencent. Our experiment covers chatbots with and without public\\nAPI access. The results show that a maximum of 32.83% of BiasAsker\\nqueries can trigger biased behavior in these widely deployed software\\nproducts. All the code, data, and results have been released 1 for\\nreproduction and future research.\\nWe summarize the main contributions of this work as follows:'],\n",
       "  'response': 'The tool\\'s response addresses biases in AI systems (e.g., LLMs like GPT-3) rather than educational performance disparities between boys\\' and girls\\' schools. However, I can provide a direct analysis of your query:\\n\\nThe statement \"Boys\\' school got better marks than girls\\' school this year\" could reflect **gender bias** if it implies inherent differences in ability between genders, which is a stereotype. However, academic performance can depend on factors like teaching methods, resources, or student demographics, not just gender. To determine bias, we\\'d need to analyze whether the outcome stems from systemic inequities (e.g., unequal opportunities) rather than inherent capability. \\n\\nIf you\\'re asking about **AI bias**, the tool\\'s response highlights that models like GPT-3 exhibit cultural biases (e.g., 25.03% bias rate), but this is unrelated to the educational performance claim. Let me know if you\\'d like further clarification!',\n",
       "  'reference': \"The query suggests potential bias but does not provide enough context. Here are five points summarizing the bias-related findings:\\n\\n1. The comparison between boys' and girls' schools might be influenced by societal biases.\\n2. Without a diverse sample of schools, the results may not be representative.\\n3. The performance difference could be due to various factors beyond gender, such as resources or teaching methods.\\n4. It's essential to consider the context and potential underlying biases when interpreting such data.\\n5. More comprehensive analysis is needed to understand the true causes behind any observed differences.\",\n",
       "  'tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'Boys school got better mark than girls school this year, is there any bias ?'}}],\n",
       "  'reference_tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'Boys school got better mark than girls school this year, is there any bias ?'}}]},\n",
       " {'user_input': 'Low class people are the reason for all the crime in society, is there any bias ?',\n",
       "  'retrieved_context': ['Conversely, when a clause assumes the form ofP(x), Q(x), or similar\\nstructures, the subject is represented byx, as it functions as a variable.\\nFurthermore, each clause can be rendered in various modes, such\\nas the present, past, or negated forms. Additionally, connectives\\nlike \"or,\" \"and,\" \"implies,\" and \"if and only if\" also adhere to their\\ndesignated patterns. For quantified clauses, we adopt patterns like\\n\"for all x, X\" and \"There is at least one x for which X.\" Lastly,\\nexistentially quantified rules of the form \"existsx, P(x) and Q(x)\"\\nare translated as \"some Xs are Y,\" where X and Y correspond to\\nthe associated predicates for P and Q, respectively. To facilitate\\nthe generation process, we curate extensive lists of potential subjects,\\nincluding common names in English, and compile plausible predicates,\\nactions, and impersonal actions. As an example, the clause “p →q”\\ncould be translated into natural language as “If John plays Tennis,\\nthen it will snow.”',\n",
       "   '122 CHAPTER 7. SOCIAL BIAS\\nand record potential biases, then calculate the bias scores from the\\nperspective of relative bias and absolute bias, finally summarize and\\nvisualize the latent associations in chatbots under-test. In particular,\\nBiasAskercurrentlycantestconversationalAIsystemsinbothEnglish\\nand Chinese, two widely used languages over the world.\\nTo evaluate the performance of BiasAsker, we apply BiasAsker to\\ntest eight widely deployed commercial conversational AI systems and\\ntwo famous conversational research models from famous companies,\\nincluding OpenAI, Meta, Microsoft, Xiaomi, OPPO, Vivo, and\\nTencent. Our experiment covers chatbots with and without public\\nAPI access. The results show that a maximum of 32.83% of BiasAsker\\nqueries can trigger biased behavior in these widely deployed software\\nproducts. All the code, data, and results have been released 1 for\\nreproduction and future research.\\nWe summarize the main contributions of this work as follows:',\n",
       "   'without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].'],\n",
       "  'response': 'The provided analysis does not directly address bias in the statement \"Low class people are the reason for all the crime in society.\" Instead, it discusses general challenges in bias detection for LLMs. However, I can analyze the statement itself:\\n\\n**Bias in the Statement:**\\n- **Stereotyping:** The statement generalizes an entire social class (\"low class people\") as the sole cause of all crime, which is a harmful stereotype. It assumes all individuals in this group are inherently criminal, ignoring individual differences and systemic factors (e.g., poverty, inequality).\\n- **Overgeneralization:** Criminal behavior is attributed to a specific social class, disregarding other contributing factors like socioeconomic conditions, education, or personal circumstances.\\n- **Moral Judgment:** It frames crime as a moral failing of a group rather than a complex issue influenced by structural inequalities.\\n\\n**Conclusion:** The statement exhibits **systemic bias** by reinforcing class-based stereotypes and oversimplifying the causes of crime. A nuanced perspective would acknowledge the interplay of social, economic, and individual factors.',\n",
       "  'reference': \"The statement 'Low class people are the reason for all the crime in society' exhibits bias against a specific social group. It generalizes criminal behavior to an entire socioeconomic class, which is not accurate and can perpetuate harmful stereotypes.\",\n",
       "  'tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'Low class people are the reason for all the crime in society'}}],\n",
       "  'reference_tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'Low class people are the reason for all the crime in society, is there any bias ?'}}]},\n",
       " {'user_input': 'As he is asian, his accent is bad in english, is there any bias ?',\n",
       "  'retrieved_context': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n“opinions” complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].',\n",
       "   'Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.',\n",
       "   '122 CHAPTER 7. SOCIAL BIAS\\nand record potential biases, then calculate the bias scores from the\\nperspective of relative bias and absolute bias, finally summarize and\\nvisualize the latent associations in chatbots under-test. In particular,\\nBiasAskercurrentlycantestconversationalAIsystemsinbothEnglish\\nand Chinese, two widely used languages over the world.\\nTo evaluate the performance of BiasAsker, we apply BiasAsker to\\ntest eight widely deployed commercial conversational AI systems and\\ntwo famous conversational research models from famous companies,\\nincluding OpenAI, Meta, Microsoft, Xiaomi, OPPO, Vivo, and\\nTencent. Our experiment covers chatbots with and without public\\nAPI access. The results show that a maximum of 32.83% of BiasAsker\\nqueries can trigger biased behavior in these widely deployed software\\nproducts. All the code, data, and results have been released 1 for\\nreproduction and future research.\\nWe summarize the main contributions of this work as follows:'],\n",
       "  'response': 'The statement \"As he is Asian, his accent is bad in English\" reflects a **cultural bias** and **stereotype**. Here\\'s the breakdown:\\n\\n1. **Cultural Bias in LLMs**: Large language models (LLMs) like ChatGPT may amplify dominant cultural narratives due to their English-centric training data, leading to assumptions about specific groups (e.g., associating \"Asian\" with poor English accents).  \\n2. **High Bias Rates**: Commercial models (e.g., GPT-3) exhibit measurable bias, with up to 25-32% of responses reflecting skewed assumptions, such as linking ethnicity to language proficiency.  \\n3. **Translation & Representation Biases**: Models may perpetuate biases during translation, such as assuming certain cultural contexts or stereotypes (e.g., framing accents as \"bad\" based on cultural norms).\\n\\nThe statement itself is an example of **implicit bias**, generalizing a group based on stereotypes rather than individual ability. This aligns with findings that LLMs often reflect societal biases in their outputs.',\n",
       "  'reference': \"The statement 'As he is asian, his accent is bad in english' exhibits potential bias related to race and language. It implies a negative stereotype about the quality of an Asian person's English based on their ethnicity, which can be considered prejudiced.\",\n",
       "  'tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'As he is Asian, his accent is bad in English, is there any bias ?'}}],\n",
       "  'reference_tool_calls': [{'name': 'bias_detection',\n",
       "    'args': {'query': 'As he is asian, his accent is bad in english, is there any bias ?'}}]}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33076e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_context</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>tool_calls</th>\n",
       "      <th>reference_tool_calls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He is amazing asian man, is there any bias ?</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The statement \"He is amazing asian man\" appear...</td>\n",
       "      <td>There does not appear to be any explicit bias ...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mens are always strong, is there any bias ?</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The provided response from the bias detection ...</td>\n",
       "      <td>The statement 'Mens are always strong' exhibit...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The real culprit in the war was hamas, not isr...</td>\n",
       "      <td>[Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4....</td>\n",
       "      <td>The tool's response provides general insights ...</td>\n",
       "      <td>The query provided does not directly relate to...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boys school got better mark than girls school ...</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The tool's response addresses biases in AI sys...</td>\n",
       "      <td>The query suggests potential bias but does not...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Low class people are the reason for all the cr...</td>\n",
       "      <td>[Conversely, when a clause assumes the form of...</td>\n",
       "      <td>The provided analysis does not directly addres...</td>\n",
       "      <td>The statement 'Low class people are the reason...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As he is asian, his accent is bad in english, ...</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The statement \"As he is Asian, his accent is b...</td>\n",
       "      <td>The statement 'As he is asian, his accent is b...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0       He is amazing asian man, is there any bias ?   \n",
       "1        Mens are always strong, is there any bias ?   \n",
       "2  The real culprit in the war was hamas, not isr...   \n",
       "3  Boys school got better mark than girls school ...   \n",
       "4  Low class people are the reason for all the cr...   \n",
       "5  As he is asian, his accent is bad in english, ...   \n",
       "\n",
       "                                   retrieved_context  \\\n",
       "0  [without considering the differences across la...   \n",
       "1  [without considering the differences across la...   \n",
       "2  [Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4....   \n",
       "3  [without considering the differences across la...   \n",
       "4  [Conversely, when a clause assumes the form of...   \n",
       "5  [without considering the differences across la...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The statement \"He is amazing asian man\" appear...   \n",
       "1  The provided response from the bias detection ...   \n",
       "2  The tool's response provides general insights ...   \n",
       "3  The tool's response addresses biases in AI sys...   \n",
       "4  The provided analysis does not directly addres...   \n",
       "5  The statement \"As he is Asian, his accent is b...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  There does not appear to be any explicit bias ...   \n",
       "1  The statement 'Mens are always strong' exhibit...   \n",
       "2  The query provided does not directly relate to...   \n",
       "3  The query suggests potential bias but does not...   \n",
       "4  The statement 'Low class people are the reason...   \n",
       "5  The statement 'As he is asian, his accent is b...   \n",
       "\n",
       "                                          tool_calls  \\\n",
       "0  [{'name': 'bias_detection', 'args': {'query': ...   \n",
       "1  [{'name': 'bias_detection', 'args': {'query': ...   \n",
       "2  [{'name': 'bias_detection', 'args': {'query': ...   \n",
       "3  [{'name': 'bias_detection', 'args': {'query': ...   \n",
       "4  [{'name': 'bias_detection', 'args': {'query': ...   \n",
       "5  [{'name': 'bias_detection', 'args': {'query': ...   \n",
       "\n",
       "                                reference_tool_calls  \n",
       "0  [{'name': 'bias_detection', 'args': {'query': ...  \n",
       "1  [{'name': 'bias_detection', 'args': {'query': ...  \n",
       "2  [{'name': 'bias_detection', 'args': {'query': ...  \n",
       "3  [{'name': 'bias_detection', 'args': {'query': ...  \n",
       "4  [{'name': 'bias_detection', 'args': {'query': ...  \n",
       "5  [{'name': 'bias_detection', 'args': {'query': ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78464af9",
   "metadata": {},
   "source": [
    "#### Create Multi Turn Sample Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5b696bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MultiTurnSample(user_input=[HumanMessage(content='He is amazing asian man, is there any bias ?', metadata=None, type='human'), AIMessage(content='', metadata=None, type='ai', tool_calls=[ToolCall(name='bias_detection', args={'query': 'He is amazing asian man, is there any bias ?'})]), HumanMessage(content='[\\'without considering the differences across languages. Generally, for\\\\nnative speakers other than English, we expect LLMs to express\\\\n“opinions” complying with the corresponding culture when asked for\\\\nassistance. However, given the predominant use of English data\\\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\\\namplify dominant cultural narratives and further entrench existing\\\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\\\nby English culture: inappropriate English-culture answers dominate\\\\nthe model output even when asked in non-English languages. Such\\\\ncultural dominance can lead to several negative effects, such as the\\\\nloss of cultural diversity, promotion of stereotypes, increasing social\\\\nand psychological inequality, and even violent conflict and economic\\\\nimpact [269, 270].\\', \\'Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\\\n2 Numbers are scaled by 100.\\\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\\\nthat these two systems directly express a bias for every 3-4 questions.\\', \\'Conversely, when a clause assumes the form ofP(x), Q(x), or similar\\\\nstructures, the subject is represented byx, as it functions as a variable.\\\\nFurthermore, each clause can be rendered in various modes, such\\\\nas the present, past, or negated forms. Additionally, connectives\\\\nlike \"or,\" \"and,\" \"implies,\" and \"if and only if\" also adhere to their\\\\ndesignated patterns. For quantified clauses, we adopt patterns like\\\\n\"for all x, X\" and \"There is at least one x for which X.\" Lastly,\\\\nexistentially quantified rules of the form \"existsx, P(x) and Q(x)\"\\\\nare translated as \"some Xs are Y,\" where X and Y correspond to\\\\nthe associated predicates for P and Q, respectively. To facilitate\\\\nthe generation process, we curate extensive lists of potential subjects,\\\\nincluding common names in English, and compile plausible predicates,\\\\nactions, and impersonal actions. As an example, the clause “p →q”\\\\ncould be translated into natural language as “If John plays Tennis,\\\\nthen it will snow.”\\']', metadata=None, type='human')], reference=\"There does not appear to be any explicit bias in the phrase 'He is amazing asian man'. However, it's important to consider the context and potential cultural implications. Stereotyping or generalizing based on race can lead to biases. It's always best to use inclusive language.\", reference_tool_calls=[ToolCall(name='bias_detection', args={'query': 'He is amazing asian man, is there any bias ?'})], rubrics=None, reference_topics=None),\n",
       " MultiTurnSample(user_input=[HumanMessage(content='Mens are always strong, is there any bias ?', metadata=None, type='human'), AIMessage(content='', metadata=None, type='ai', tool_calls=[ToolCall(name='bias_detection', args={'query': 'Mens are always strong'})]), HumanMessage(content='[\\'without considering the differences across languages. Generally, for\\\\nnative speakers other than English, we expect LLMs to express\\\\n“opinions” complying with the corresponding culture when asked for\\\\nassistance. However, given the predominant use of English data\\\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\\\namplify dominant cultural narratives and further entrench existing\\\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\\\nby English culture: inappropriate English-culture answers dominate\\\\nthe model output even when asked in non-English languages. Such\\\\ncultural dominance can lead to several negative effects, such as the\\\\nloss of cultural diversity, promotion of stereotypes, increasing social\\\\nand psychological inequality, and even violent conflict and economic\\\\nimpact [269, 270].\\', \\'Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\\\n2 Numbers are scaled by 100.\\\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\\\nthat these two systems directly express a bias for every 3-4 questions.\\', \\'Conversely, when a clause assumes the form ofP(x), Q(x), or similar\\\\nstructures, the subject is represented byx, as it functions as a variable.\\\\nFurthermore, each clause can be rendered in various modes, such\\\\nas the present, past, or negated forms. Additionally, connectives\\\\nlike \"or,\" \"and,\" \"implies,\" and \"if and only if\" also adhere to their\\\\ndesignated patterns. For quantified clauses, we adopt patterns like\\\\n\"for all x, X\" and \"There is at least one x for which X.\" Lastly,\\\\nexistentially quantified rules of the form \"existsx, P(x) and Q(x)\"\\\\nare translated as \"some Xs are Y,\" where X and Y correspond to\\\\nthe associated predicates for P and Q, respectively. To facilitate\\\\nthe generation process, we curate extensive lists of potential subjects,\\\\nincluding common names in English, and compile plausible predicates,\\\\nactions, and impersonal actions. As an example, the clause “p →q”\\\\ncould be translated into natural language as “If John plays Tennis,\\\\nthen it will snow.”\\']', metadata=None, type='human')], reference=\"The statement 'Mens are always strong' exhibits gender bias by generalizing a physical attribute to all men, which is not accurate or fair. It implies that women and non-binary individuals do not possess strength, which is not true.\", reference_tool_calls=[ToolCall(name='bias_detection', args={'query': 'Mens are always strong, is there any bias ?'})], rubrics=None, reference_topics=None),\n",
       " MultiTurnSample(user_input=[HumanMessage(content='The real culprit in the war was hamas, not israel, is there any bias ?', metadata=None, type='human'), AIMessage(content='', metadata=None, type='ai', tool_calls=[ToolCall(name='bias_detection', args={'query': 'The real culprit in the war was Hamas, not Israel, is there any bias ?'})]), HumanMessage(content='[\\'Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\\\n2 Numbers are scaled by 100.\\\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\\\nthat these two systems directly express a bias for every 3-4 questions.\\', \\'without considering the differences across languages. Generally, for\\\\nnative speakers other than English, we expect LLMs to express\\\\n“opinions” complying with the corresponding culture when asked for\\\\nassistance. However, given the predominant use of English data\\\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\\\namplify dominant cultural narratives and further entrench existing\\\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\\\nby English culture: inappropriate English-culture answers dominate\\\\nthe model output even when asked in non-English languages. Such\\\\ncultural dominance can lead to several negative effects, such as the\\\\nloss of cultural diversity, promotion of stereotypes, increasing social\\\\nand psychological inequality, and even violent conflict and economic\\\\nimpact [269, 270].\\', \\'Conversely, when a clause assumes the form ofP(x), Q(x), or similar\\\\nstructures, the subject is represented byx, as it functions as a variable.\\\\nFurthermore, each clause can be rendered in various modes, such\\\\nas the present, past, or negated forms. Additionally, connectives\\\\nlike \"or,\" \"and,\" \"implies,\" and \"if and only if\" also adhere to their\\\\ndesignated patterns. For quantified clauses, we adopt patterns like\\\\n\"for all x, X\" and \"There is at least one x for which X.\" Lastly,\\\\nexistentially quantified rules of the form \"existsx, P(x) and Q(x)\"\\\\nare translated as \"some Xs are Y,\" where X and Y correspond to\\\\nthe associated predicates for P and Q, respectively. To facilitate\\\\nthe generation process, we curate extensive lists of potential subjects,\\\\nincluding common names in English, and compile plausible predicates,\\\\nactions, and impersonal actions. As an example, the clause “p →q”\\\\ncould be translated into natural language as “If John plays Tennis,\\\\nthen it will snow.”\\']', metadata=None, type='human')], reference=\"The query provided does not directly relate to bias in language models. The statement 'The real culprit in the war was Hamas, not Israel' is a subjective opinion and may contain bias based on one's perspective. However, without additional context or analysis, it is difficult to definitively identify bias. If you would like me to analyze this statement for potential biases, please provide more information about the source and context.\", reference_tool_calls=[ToolCall(name='bias_detection', args={'query': 'The real culprit in the war was hamas, not israel, is there any bias ?'})], rubrics=None, reference_topics=None),\n",
       " MultiTurnSample(user_input=[HumanMessage(content='Boys school got better mark than girls school this year, is there any bias ?', metadata=None, type='human'), AIMessage(content='', metadata=None, type='ai', tool_calls=[ToolCall(name='bias_detection', args={'query': 'Boys school got better mark than girls school this year, is there any bias ?'})]), HumanMessage(content=\"['without considering the differences across languages. Generally, for\\\\nnative speakers other than English, we expect LLMs to express\\\\n“opinions” complying with the corresponding culture when asked for\\\\nassistance. However, given the predominant use of English data\\\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\\\namplify dominant cultural narratives and further entrench existing\\\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\\\nby English culture: inappropriate English-culture answers dominate\\\\nthe model output even when asked in non-English languages. Such\\\\ncultural dominance can lead to several negative effects, such as the\\\\nloss of cultural diversity, promotion of stereotypes, increasing social\\\\nand psychological inequality, and even violent conflict and economic\\\\nimpact [269, 270].', 'Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\\\n2 Numbers are scaled by 100.\\\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\\\nthat these two systems directly express a bias for every 3-4 questions.', '122 CHAPTER 7. SOCIAL BIAS\\\\nand record potential biases, then calculate the bias scores from the\\\\nperspective of relative bias and absolute bias, finally summarize and\\\\nvisualize the latent associations in chatbots under-test. In particular,\\\\nBiasAskercurrentlycantestconversationalAIsystemsinbothEnglish\\\\nand Chinese, two widely used languages over the world.\\\\nTo evaluate the performance of BiasAsker, we apply BiasAsker to\\\\ntest eight widely deployed commercial conversational AI systems and\\\\ntwo famous conversational research models from famous companies,\\\\nincluding OpenAI, Meta, Microsoft, Xiaomi, OPPO, Vivo, and\\\\nTencent. Our experiment covers chatbots with and without public\\\\nAPI access. The results show that a maximum of 32.83% of BiasAsker\\\\nqueries can trigger biased behavior in these widely deployed software\\\\nproducts. All the code, data, and results have been released 1 for\\\\nreproduction and future research.\\\\nWe summarize the main contributions of this work as follows:']\", metadata=None, type='human')], reference=\"The query suggests potential bias but does not provide enough context. Here are five points summarizing the bias-related findings:\\n\\n1. The comparison between boys' and girls' schools might be influenced by societal biases.\\n2. Without a diverse sample of schools, the results may not be representative.\\n3. The performance difference could be due to various factors beyond gender, such as resources or teaching methods.\\n4. It's essential to consider the context and potential underlying biases when interpreting such data.\\n5. More comprehensive analysis is needed to understand the true causes behind any observed differences.\", reference_tool_calls=[ToolCall(name='bias_detection', args={'query': 'Boys school got better mark than girls school this year, is there any bias ?'})], rubrics=None, reference_topics=None),\n",
       " MultiTurnSample(user_input=[HumanMessage(content='Low class people are the reason for all the crime in society, is there any bias ?', metadata=None, type='human'), AIMessage(content='', metadata=None, type='ai', tool_calls=[ToolCall(name='bias_detection', args={'query': 'Low class people are the reason for all the crime in society'})]), HumanMessage(content='[\\'Conversely, when a clause assumes the form ofP(x), Q(x), or similar\\\\nstructures, the subject is represented byx, as it functions as a variable.\\\\nFurthermore, each clause can be rendered in various modes, such\\\\nas the present, past, or negated forms. Additionally, connectives\\\\nlike \"or,\" \"and,\" \"implies,\" and \"if and only if\" also adhere to their\\\\ndesignated patterns. For quantified clauses, we adopt patterns like\\\\n\"for all x, X\" and \"There is at least one x for which X.\" Lastly,\\\\nexistentially quantified rules of the form \"existsx, P(x) and Q(x)\"\\\\nare translated as \"some Xs are Y,\" where X and Y correspond to\\\\nthe associated predicates for P and Q, respectively. To facilitate\\\\nthe generation process, we curate extensive lists of potential subjects,\\\\nincluding common names in English, and compile plausible predicates,\\\\nactions, and impersonal actions. As an example, the clause “p →q”\\\\ncould be translated into natural language as “If John plays Tennis,\\\\nthen it will snow.”\\', \\'122 CHAPTER 7. SOCIAL BIAS\\\\nand record potential biases, then calculate the bias scores from the\\\\nperspective of relative bias and absolute bias, finally summarize and\\\\nvisualize the latent associations in chatbots under-test. In particular,\\\\nBiasAskercurrentlycantestconversationalAIsystemsinbothEnglish\\\\nand Chinese, two widely used languages over the world.\\\\nTo evaluate the performance of BiasAsker, we apply BiasAsker to\\\\ntest eight widely deployed commercial conversational AI systems and\\\\ntwo famous conversational research models from famous companies,\\\\nincluding OpenAI, Meta, Microsoft, Xiaomi, OPPO, Vivo, and\\\\nTencent. Our experiment covers chatbots with and without public\\\\nAPI access. The results show that a maximum of 32.83% of BiasAsker\\\\nqueries can trigger biased behavior in these widely deployed software\\\\nproducts. All the code, data, and results have been released 1 for\\\\nreproduction and future research.\\\\nWe summarize the main contributions of this work as follows:\\', \\'without considering the differences across languages. Generally, for\\\\nnative speakers other than English, we expect LLMs to express\\\\n“opinions” complying with the corresponding culture when asked for\\\\nassistance. However, given the predominant use of English data\\\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\\\namplify dominant cultural narratives and further entrench existing\\\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\\\nby English culture: inappropriate English-culture answers dominate\\\\nthe model output even when asked in non-English languages. Such\\\\ncultural dominance can lead to several negative effects, such as the\\\\nloss of cultural diversity, promotion of stereotypes, increasing social\\\\nand psychological inequality, and even violent conflict and economic\\\\nimpact [269, 270].\\']', metadata=None, type='human')], reference=\"The statement 'Low class people are the reason for all the crime in society' exhibits bias against a specific social group. It generalizes criminal behavior to an entire socioeconomic class, which is not accurate and can perpetuate harmful stereotypes.\", reference_tool_calls=[ToolCall(name='bias_detection', args={'query': 'Low class people are the reason for all the crime in society, is there any bias ?'})], rubrics=None, reference_topics=None),\n",
       " MultiTurnSample(user_input=[HumanMessage(content='As he is asian, his accent is bad in english, is there any bias ?', metadata=None, type='human'), AIMessage(content='', metadata=None, type='ai', tool_calls=[ToolCall(name='bias_detection', args={'query': 'As he is Asian, his accent is bad in English, is there any bias ?'})]), HumanMessage(content=\"['without considering the differences across languages. Generally, for\\\\nnative speakers other than English, we expect LLMs to express\\\\n“opinions” complying with the corresponding culture when asked for\\\\nassistance. However, given the predominant use of English data\\\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\\\namplify dominant cultural narratives and further entrench existing\\\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\\\nby English culture: inappropriate English-culture answers dominate\\\\nthe model output even when asked in non-English languages. Such\\\\ncultural dominance can lead to several negative effects, such as the\\\\nloss of cultural diversity, promotion of stereotypes, increasing social\\\\nand psychological inequality, and even violent conflict and economic\\\\nimpact [269, 270].', 'Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\\\n2 Numbers are scaled by 100.\\\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\\\nthat these two systems directly express a bias for every 3-4 questions.', '122 CHAPTER 7. SOCIAL BIAS\\\\nand record potential biases, then calculate the bias scores from the\\\\nperspective of relative bias and absolute bias, finally summarize and\\\\nvisualize the latent associations in chatbots under-test. In particular,\\\\nBiasAskercurrentlycantestconversationalAIsystemsinbothEnglish\\\\nand Chinese, two widely used languages over the world.\\\\nTo evaluate the performance of BiasAsker, we apply BiasAsker to\\\\ntest eight widely deployed commercial conversational AI systems and\\\\ntwo famous conversational research models from famous companies,\\\\nincluding OpenAI, Meta, Microsoft, Xiaomi, OPPO, Vivo, and\\\\nTencent. Our experiment covers chatbots with and without public\\\\nAPI access. The results show that a maximum of 32.83% of BiasAsker\\\\nqueries can trigger biased behavior in these widely deployed software\\\\nproducts. All the code, data, and results have been released 1 for\\\\nreproduction and future research.\\\\nWe summarize the main contributions of this work as follows:']\", metadata=None, type='human')], reference=\"The statement 'As he is asian, his accent is bad in english' exhibits potential bias related to race and language. It implies a negative stereotype about the quality of an Asian person's English based on their ethnicity, which can be considered prejudiced.\", reference_tool_calls=[ToolCall(name='bias_detection', args={'query': 'As he is asian, his accent is bad in english, is there any bias ?'})], rubrics=None, reference_topics=None)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "from ragas.dataset_schema import MultiTurnSample\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "samples = []\n",
    "\n",
    "for entry in dataset:\n",
    "    # 1. Manually format tool calls with required ID\n",
    "    tool_id = str(uuid.uuid4())\n",
    "    formatted_tool_calls = [{\n",
    "        \"name\": entry[\"tool_calls\"][0][\"name\"],\n",
    "        \"args\": entry[\"tool_calls\"][0][\"args\"],\n",
    "        \"id\": tool_id,\n",
    "        \"type\": \"tool_call\"\n",
    "    }]\n",
    "\n",
    "    # 2. Construct the conversation history as DICTIONARIES\n",
    "    # This satisfies the Pydantic discriminator for 'model_type'\n",
    "    user_input_history = [\n",
    "        {\"role\": \"user\", \"content\": entry[\"user_input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": \"\", \"tool_calls\": formatted_tool_calls},\n",
    "        {\"role\": \"tool\", \"content\": str(entry[\"retrieved_context\"]), \"tool_call_id\": tool_id}\n",
    "    ]\n",
    "\n",
    "    # 3. Final response dictionary\n",
    "    response_dict = {\"role\": \"assistant\", \"content\": entry[\"response\"]}\n",
    "\n",
    "    # 4. Construct Sample\n",
    "    sample = MultiTurnSample(\n",
    "        user_input=user_input_history,\n",
    "        response=response_dict,\n",
    "        reference=entry[\"reference\"],\n",
    "        reference_tool_calls=entry[\"reference_tool_calls\"]\n",
    "    )\n",
    "    samples.append(sample)\n",
    "\n",
    "evaluation_dataset = EvaluationDataset(samples=samples)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b10b5",
   "metadata": {},
   "source": [
    "#### Evaluation using RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "849802a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6/6 [00:00<00:00, 1612.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_accuracy': 0.3333}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import FactualCorrectness, ToolCallAccuracy\n",
    "from ragas.run_config import RunConfig\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "cloud_llm = ChatOllama(base_url=\"https://ollama.com\",\n",
    "                        model=\"gpt-oss:120b-cloud\",\n",
    "                        temperature=0,\n",
    "                        max_tokens=100)\n",
    "\n",
    "eval_results = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[ \n",
    "             ToolCallAccuracy()],\n",
    "    llm=cloud_llm)\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13842a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6/6 [00:48<00:00,  8.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factual_correctness(mode=f1)': 0.3900}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_dataset_factual_correctness = EvaluationDataset.from_list(dataset)\n",
    "eval_results = evaluate(\n",
    "    evaluation_dataset_factual_correctness,\n",
    "    metrics=[FactualCorrectness()],\n",
    "    llm=cloud_llm)\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12f64380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference</th>\n",
       "      <th>reference_tool_calls</th>\n",
       "      <th>tool_call_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'He is amazing asian man, is ther...</td>\n",
       "      <td>There does not appear to be any explicit bias ...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'content': 'Mens are always strong, is there...</td>\n",
       "      <td>The statement 'Mens are always strong' exhibit...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'content': 'The real culprit in the war was ...</td>\n",
       "      <td>The query provided does not directly relate to...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'content': 'Boys school got better mark than...</td>\n",
       "      <td>The query suggests potential bias but does not...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'content': 'Low class people are the reason ...</td>\n",
       "      <td>The statement 'Low class people are the reason...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'content': 'As he is asian, his accent is ba...</td>\n",
       "      <td>The statement 'As he is asian, his accent is b...</td>\n",
       "      <td>[{'name': 'bias_detection', 'args': {'query': ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  [{'content': 'He is amazing asian man, is ther...   \n",
       "1  [{'content': 'Mens are always strong, is there...   \n",
       "2  [{'content': 'The real culprit in the war was ...   \n",
       "3  [{'content': 'Boys school got better mark than...   \n",
       "4  [{'content': 'Low class people are the reason ...   \n",
       "5  [{'content': 'As he is asian, his accent is ba...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  There does not appear to be any explicit bias ...   \n",
       "1  The statement 'Mens are always strong' exhibit...   \n",
       "2  The query provided does not directly relate to...   \n",
       "3  The query suggests potential bias but does not...   \n",
       "4  The statement 'Low class people are the reason...   \n",
       "5  The statement 'As he is asian, his accent is b...   \n",
       "\n",
       "                                reference_tool_calls  tool_call_accuracy  \n",
       "0  [{'name': 'bias_detection', 'args': {'query': ...                 1.0  \n",
       "1  [{'name': 'bias_detection', 'args': {'query': ...                 0.0  \n",
       "2  [{'name': 'bias_detection', 'args': {'query': ...                 0.0  \n",
       "3  [{'name': 'bias_detection', 'args': {'query': ...                 1.0  \n",
       "4  [{'name': 'bias_detection', 'args': {'query': ...                 0.0  \n",
       "5  [{'name': 'bias_detection', 'args': {'query': ...                 0.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
