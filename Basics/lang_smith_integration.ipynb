{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7b219d",
   "metadata": {},
   "source": [
    "#### Interacting with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90872d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings or emotions. But I'm here and ready to help you with any questions or tasks you might have. How can I assist you today?\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-12-08T20:38:30.862003Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2996855834, 'load_duration': 95806084, 'prompt_eval_count': 36, 'prompt_eval_duration': 337823750, 'eval_count': 44, 'eval_duration': 2218275046, 'logprobs': None, 'model_name': 'qwen2.5:latest', 'model_provider': 'ollama'}, id='lc_run--019affaf-e117-70b3-a330-747f8e9399bc-0', usage_metadata={'input_tokens': 36, 'output_tokens': 44, 'total_tokens': 80})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "local_llm = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "                       model=\"qwen2.5:latest\",\n",
    "                       temperature=0.5,\n",
    "                       max_token=250)\n",
    "local_llm.invoke(\"Hello, how are you today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250e0cd",
   "metadata": {},
   "source": [
    "#### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083a817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Running Large Language Models (LLMs) on your local machine can offer several advantages, depending on your specific use case and requirements. Here are some key benefits:\\n\\n1. **Control and Privacy**: Running a model locally means you have complete control over the data and can ensure that sensitive information is not exposed to external entities.\\n\\n2. **Customization**: You can tailor the model's behavior by modifying configurations or fine-tuning it on your own dataset without relying on cloud services, which might limit customization options.\\n\\n3. **Data Security**: Local execution allows you to avoid potential security risks associated with transmitting data over the internet to remote servers. This is particularly important for handling sensitive or regulated information.\\n\\n4. **Latency Reduction**: For applications that require real-time responses, running a model locally can significantly reduce latency compared to sending requests to a cloud service and waiting for responses.\\n\\n5. **Cost Efficiency**: Depending on your use case, local execution might be more cost-effective than paying for cloud services, especially if you only need the model occasionally or for short periods.\\n\\n6. **Offline Usage**: You can use the model even when you don't have an internet connection, making it useful in scenarios where connectivity is unreliable or non-existent.\\n\\n7. **Deployment Flexibility**: Locally running a model allows for more flexible deployment options, such as integrating with existing local infrastructure or deploying on edge devices.\\n\\n8. **Resource Management**: You can optimize resource usage by managing the hardware and software environment yourself, potentially leading to better performance and efficiency.\\n\\nHowever, it's important to note that running large language models locally also has some disadvantages, including higher computational and storage requirements, potential legal and ethical considerations related to data privacy, and the complexity of setting up and maintaining a powerful local computing environment.\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-12-08T20:38:18.953368Z', 'done': True, 'done_reason': 'stop', 'total_duration': 21516807167, 'load_duration': 94964209, 'prompt_eval_count': 40, 'prompt_eval_duration': 141454167, 'eval_count': 356, 'eval_duration': 18827701585, 'logprobs': None, 'model_name': 'qwen2.5:latest', 'model_provider': 'ollama'}, id='lc_run--019affaf-6a3b-76d3-a3b3-db0a229e600b-0', usage_metadata={'input_tokens': 40, 'output_tokens': 356, 'total_tokens': 396})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# Structured prompt\n",
    "prompt_template = PromptTemplate.from_template(\"What is the advantage of running LLM in {environment}\")\n",
    "prompt = prompt_template.invoke(input={\"environment\": \"Local Machine\"})\n",
    "local_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d6a055",
   "metadata": {},
   "source": [
    "#### Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9144d37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Running a Large Language Model (LLM) on your local machine has several advantages, but it also comes with certain limitations. Here are some key benefits:\\n\\n1. **Control and Privacy**: Running an LLM locally gives you full control over the data and processing. This can be crucial for sensitive or confidential information that you don't want to store in cloud services.\\n\\n2. **Latency Reduction**: Local execution reduces latency since there's no need to send requests over a network to reach remote servers. This is particularly beneficial for real-time applications where response time is critical.\\n\\n3. **Customization and Flexibility**: You can tailor the model's behavior more closely to your specific needs by modifying configurations, adding custom prompts, or integrating it with other local applications.\\n\\n4. **Cost-Effectiveness**: For small-scale projects or personal use, running an LLM locally might be more cost-effective than paying for cloud services, especially if you only need occasional usage.\\n\\n5. **Offline Access**: You can use the model even without internet connectivity, which is useful in scenarios where network access is unreliable or restricted.\\n\\n6. **Resource Management**: You have direct control over the hardware resources (CPU/GPU) used by the LLM, allowing for more efficient resource allocation and potentially better performance tuning.\\n\\nHowever, it's important to note that running a large language model on your local machine also has some downsides:\\n\\n- **Storage Requirements**: Large models require significant storage space. For example, a typical LLM might need several gigabytes of disk space.\\n- **Computational Power**: Running such models often requires powerful hardware (e.g., GPUs), which can be expensive and may not always be available on personal machines.\\n- **Scalability Limitations**: Local setups are limited by the physical constraints of your machine, making it difficult to scale up for very large or complex tasks.\\n\\nIn summary, while running an LLM locally offers several advantages in terms of control, performance, and privacy, it is not always feasible due to resource requirements. The decision to run a model locally should be based on specific use case needs and constraints.\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-12-08T21:09:58.608079Z', 'done': True, 'done_reason': 'stop', 'total_duration': 27978276958, 'load_duration': 99090000, 'prompt_eval_count': 30, 'prompt_eval_duration': 1757559208, 'eval_count': 426, 'eval_duration': 23111087038, 'logprobs': None, 'model_name': 'qwen2.5:latest', 'model_provider': 'ollama'}, id='lc_run--019affcc-4d7d-7741-b47e-ec66b0f13e49-0', usage_metadata={'input_tokens': 30, 'output_tokens': 426, 'total_tokens': 456})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    HumanMessagePromptTemplate, \n",
    "    SystemMessagePromptTemplate\n",
    "    )\n",
    "\n",
    "systemMessage = SystemMessagePromptTemplate.from_template(\"You are a LLM expert\")\n",
    "humanMessage = HumanMessagePromptTemplate.from_template(\"What is the advantage of running LLM in {environment}\")\n",
    "# chat_prompt_template = ChatPromptTemplate([\n",
    "#     (\"system\", \"You are a LLM expert\"),\n",
    "#     (\"user\", \"What is the advantage of running LLM in {environment}\")\n",
    "# ])\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([systemMessage,\n",
    "                                           humanMessage])\n",
    "\n",
    "chat_prompt = chat_prompt_template.invoke({\"environment\": \"Local machine\"})\n",
    "local_llm.invoke(chat_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274fa785",
   "metadata": {},
   "source": [
    "#### Message Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48495cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Running Large Language Models (LLMs) on a local machine has several advantages, but it also comes with significant limitations. Here are some key benefits:\\n\\n1. **Cost Efficiency**:\\n   - **No Cloud Costs**: Running an LLM locally eliminates ongoing cloud costs associated with using services like AWS SageMaker, Google AI Platform, or Azure Machine Learning.\\n   - **Reduced Bandwidth Usage**: You don't need to rely on internet bandwidth for data transfer and model updates.\\n\\n2. **Control and Customization**:\\n   - **Full Control Over Environment**: You have complete control over the hardware, software environment, and configuration settings.\\n   - **Customization**: You can fine-tune the LLM or modify it according to your specific needs without relying on external services.\\n\\n3. **Data Privacy and Security**:\\n   - **Local Data Processing**: You can process sensitive data locally without sending it over the internet, which enhances privacy and security.\\n   - **Compliance**: This setup is more suitable for environments with strict compliance requirements regarding data handling.\\n\\n4. **Flexibility in Deployment**:\\n   - **Offline Use**: The model can be used even when there's no internet connection or limited connectivity.\\n   - **Integration**: You can integrate the LLM into local applications and workflows without relying on external services.\\n\\n5. **Performance Optimization**:\\n   - **Customized Hardware Utilization**: You can optimize hardware usage by fine-tuning settings for your specific machine, potentially improving performance.\\n   - **Real-time Processing**: For real-time applications, running locally can reduce latency compared to cloud-based solutions.\\n\\n6. **Experimentation and Development**:\\n   - **Faster Iterations**: Local development environments allow for faster experimentation and model training iterations.\\n   - **Debugging**: Easier debugging and testing of models without the complexity of cloud infrastructure.\\n\\n### Limitations\\n\\nWhile there are many advantages, running LLMs locally also has significant drawbacks:\\n\\n1. **Hardware Requirements**:\\n   - **High-End Hardware**: Many state-of-the-art LLMs require powerful GPUs or TPUs, which can be expensive.\\n   - **Memory and Storage**: Large models consume a lot of memory and storage.\\n\\n2. **Scalability Issues**:\\n   - **Limited Scalability**: Local machines cannot scale as easily as cloud services when handling large datasets or complex tasks.\\n\\n3. **Maintenance and Updates**:\\n   - **Continuous Maintenance**: You are responsible for maintaining the software stack, including dependencies and updates.\\n   - **Model Updates**: Keeping models up-to-date can be challenging without automated tools provided by cloud services.\\n\\n4. **Expertise Required**:\\n   - **Technical Expertise**: Requires significant technical expertise to set up and maintain a local environment that can handle large language models effectively.\\n\\nIn summary, while running LLMs locally offers advantages in terms of cost, control, and security, it also comes with substantial hardware and maintenance requirements. The decision should be based on the specific use case and constraints.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a LLM Expert\"),\n",
    "    MessagesPlaceholder(\"msg\")\n",
    "])\n",
    "\n",
    "chat_prompt = chat_prompt_template.invoke({\"msg\": [HumanMessage(\"What is the advantage of running LLM in Local Machine\")]})\n",
    "local_llm.invoke(chat_prompt).content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
